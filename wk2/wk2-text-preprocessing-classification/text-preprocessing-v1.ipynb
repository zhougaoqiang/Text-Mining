{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing (week 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is prepared with the materials in the articles [Text Preprocessing in Python: Steps, Tools, and Examples]( https://www.kdnuggets.com/2018/11/text-preprocessing-python.html)\n",
    "and [Text Data Preprocessing: A Walkthrough in Python](https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html), and the materials in the article [Text Preprocessing in Python: Steps, Tools, and Examples]( https://www.kdnuggets.com/2018/11/text-preprocessing-python.html)\n",
    "\n",
    "We outline the basic steps of text preprocessing, which are needed for transferring text from human language to machine-readable format for further processing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text data preprocessing: step by step approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n"
     ]
    }
   ],
   "source": [
    "input_str = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\n",
    "input_str = input_str.lower()\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove numbers\n",
    "Remove numbers if they are not relevant to your analyses. Usually, regular expressions are used to remove numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box A contains  red and  white balls, while Box B contains  red and  blue balls.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "input_str = 'Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.'\n",
    "result = re.sub(r'\\d+', '', input_str)  # \\d+ matches a number whose length is 1 or more than 1.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation\n",
    "The following code removes this set of symbols [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This  is  an  example   of  string  with   punctuation    \n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "input_str = \"This &is [an] example? {of} string. with.? punctuation!!!!\" # Sample string\n",
    "trantab = str.maketrans(string.punctuation, \" \"*32) # the second argument has 32 space characters\n",
    "                                                    # another example, trantab = str.maketrans(\"{}\", \" \"*2)\n",
    "\n",
    "result = input_str.translate(trantab)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of string with punctuation \n"
     ]
    }
   ],
   "source": [
    "result = re.sub(r'\\s+', ' ', result)   # \\s+ matches a whitespace character whose length is 1 or more than 1.\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove whitespaces\n",
    "To remove leading and ending spaces, you can use the strip() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a string example'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = \" \\t a string example\\t \"   # \\t means tab\n",
    "input_str = input_str.strip()\n",
    "input_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Tokenization is the process of splitting the given text into smaller pieces called tokens. Words, numbers, punctuation marks, and others can be considered as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gaoqiang/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/gaoqiang/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gaoqiang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/gaoqiang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/gaoqiang/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "tokens = word_tokenize(input_str)\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer \n",
    "s = '''Good muffins cost $3.88\\nin New York. Please buy me\\ntwo of them.\\nThanks.''' \n",
    "print(TreebankWordTokenizer().tokenize(s))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(s)) # similar results as word_tokenize(); note that 'York.' -> 'York' and '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They', \"'ll\", 'save', 'and', 'invest', 'more', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"They'll save and invest more.\" \n",
    "TreebankWordTokenizer().tokenize(s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', ',', 'my', 'name', 'ca', \"n't\", 'hello', ',']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"hi, my name can't hello,\" \n",
    "TreebankWordTokenizer().tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', ',', 'my', 'name', 'can', \"'\", 't', 'hello', ',']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer, WhitespaceTokenizer\n",
    "WordPunctTokenizer().tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi,', 'my', 'name', \"can't\", 'hello,']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WhitespaceTokenizer().tokenize(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words\n",
    "“Stop words” are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts. It is possible to remove stop words using Natural Language Toolkit (NLTK), a suite of libraries and programs for symbolic and statistical natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(input_str)\n",
    "result = [i for i in tokens if not i in stop_words]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming is a process of reducing words to their word stem, base or root form (for example, books — book, looked — look). The main two algorithms are Porter stemming algorithm (removes common morphological and inflexional endings from words) and Lancaster stemming algorithm (a more aggressive stemming algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there\n",
      "are\n",
      "sever\n",
      "type\n",
      "of\n",
      "stem\n",
      "algorithm\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# use PortStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer= PorterStemmer()\n",
    "input_str=\"There are several types of stemming algorithms.\"\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ther\n",
      "ar\n",
      "sev\n",
      "typ\n",
      "of\n",
      "stem\n",
      "algorithm\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# use LancasterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer= LancasterStemmer()\n",
    "input_str=\"There are several types of stemming algorithms.\"\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "The aim of lemmatization, like stemming, is to reduce inflectional forms to a common base form. As opposed to stemming, lemmatization does not simply chop off inflections. Instead it uses lexical knowledge bases to get the correct base forms of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "been\n",
      "had\n",
      "done\n",
      "language\n",
      "city\n",
      "mouse\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "input_str=\"been had done languages cities mice\"\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text data preprocessing with a sample text\n",
    "\n",
    "We need some sample text. We'll start with something very small and artificial in order to easily see the results of what we are doing step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import contractions  # install a relevant library: $>pip install contractions\n",
    "import inflect       # $>pip install inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"\"\"<h1>Title Goes Here</h1>\n",
    "\n",
    "<b>Bolded Text</b>\n",
    "<i>Italicized Text</i>\n",
    "\n",
    "<img src=\"this should all be gone\"/>\n",
    "<a href=\"this will be gone, too\">But this will still be here!</a>\n",
    "\n",
    "I run. He ran. She is running. Will they stop running?\n",
    "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
    "\n",
    "[Some text we don't want to keep is in here]\n",
    "\n",
    "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
    "\n",
    "something... is! wrong() with.,; this :: sentence.\n",
    "\n",
    "I can't do this anymore. I didn't know them. Why couldn't you have dinner at the restaurant?\n",
    "\n",
    "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
    "\n",
    "Don't do it.... Just don't. Billy! I know what you're doing. This is a great little house you've got here.\n",
    "\n",
    "[This is some other unwanted text]\n",
    "\n",
    "John: \"Well, well, well.\"\n",
    "James: \"There, there. There, there.\"\n",
    "\n",
    "&nbsp;&nbsp;\n",
    "\n",
    "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
    "\n",
    "I have to go get 2 tutus from 2 different stores, too.\n",
    "\n",
    "22    45   1067   445\n",
    "\n",
    "{{Here is some stuff inside of double curly braces.}}\n",
    "\n",
    "{Here is more stuff in single curly braces.}\n",
    "\n",
    "[DELETE]\n",
    "\n",
    "</body>\n",
    "</html>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Removal\n",
    " \n",
    "Let's loosely define noise removal as text-specific normalization tasks which often take place prior to tokenization. While the other 2 major steps of the preprocessing framework (tokenization and normalization) are basically task-independent, noise removal is much more task-specific.\n",
    "Sample noise removal tasks could include:\n",
    "- removing text file headers, footers \n",
    "- removing HTML, XML, etc. markup and metadata \n",
    "- extracting valuable data from other formats, such as JSON \n",
    "\n",
    "Many denoising tasks, such as parsing a JSON structure, would need to be implemented prior to tokenization.\n",
    "In our data preprocessing pipeline, we will strip away HTML markup with the help of the BeautifulSoup library, and use regular expressions to remove open and close double brackets and anything in between them (we assume this is necessary based on our sample text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Goes Here\n",
      "Bolded Text\n",
      "Italicized Text\n",
      "\n",
      "But this will still be here!\n",
      "\n",
      "I run. He ran. She is running. Will they stop running?\n",
      "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
      "\n",
      "\n",
      "\n",
      "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
      "\n",
      "something... is! wrong() with.,; this :: sentence.\n",
      "\n",
      "I can't do this anymore. I didn't know them. Why couldn't you have dinner at the restaurant?\n",
      "\n",
      "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
      "\n",
      "Don't do it.... Just don't. Billy! I know what you're doing. This is a great little house you've got here.\n",
      "\n",
      "\n",
      "\n",
      "John: \"Well, well, well.\"\n",
      "James: \"There, there. There, there.\"\n",
      "\n",
      "  \n",
      "\n",
      "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
      "\n",
      "I have to go get 2 tutus from 2 different stores, too.\n",
      "\n",
      "22    45   1067   445\n",
      "\n",
      "{{Here is some stuff inside of double curly braces.}}\n",
      "\n",
      "{Here is more stuff in single curly braces.}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'\\[[^]]*\\]', '', text) # [^]]* : match anything except ']'\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text\n",
    "\n",
    "sample = denoise_text(sample)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding contractions\n",
    "While not mandatory to do at this stage prior to tokenization (you'll find that this statement is the norm for the relatively flexible ordering of text data preprocessing tasks), replacing contractions with their expansions can be beneficial at this point, since our word tokenizer will split words like \"didn't\" into \"did\" and \"n't.\" It's not impossible to remedy this tokenization at a later stage, but doing so prior makes it easier and more straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title Goes Here\n",
      "Bolded Text\n",
      "Italicized Text\n",
      "\n",
      "But this will still be here!\n",
      "\n",
      "I run. He ran. She is running. Will they stop running?\n",
      "I talked. She was talking. They talked to them about running. Who ran to the talking runner?\n",
      "\n",
      "\n",
      "\n",
      "¡Sebastián, Nicolás, Alejandro and Jéronimo are going to the store tomorrow morning!\n",
      "\n",
      "something... is! wrong() with.,; this :: sentence.\n",
      "\n",
      "I cannot do this anymore. I did not know them. Why could not you have dinner at the restaurant?\n",
      "\n",
      "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
      "\n",
      "Do not do it.... Just do not. Billy! I know what you are doing. This is a great little house you have got here.\n",
      "\n",
      "\n",
      "\n",
      "John: \"Well, well, well.\"\n",
      "James: \"There, there. There, there.\"\n",
      "\n",
      "  \n",
      "\n",
      "There are a lot of reasons not to do this. There are 101 reasons not to do it. 1000000 reasons, actually.\n",
      "\n",
      "I have to go get 2 tutus from 2 different stores, too.\n",
      "\n",
      "22    45   1067   445\n",
      "\n",
      "{{Here is some stuff inside of double curly braces.}}\n",
      "\n",
      "{Here is more stuff in single curly braces.}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "sample = replace_contractions(sample)  # e.g., can't -> can not; don't -> do not\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'Goes', 'Here', 'Bolded', 'Text', 'Italicized', 'Text', 'But', 'this', 'will', 'still', 'be', 'here', '!', 'I', 'run', '.', 'He', 'ran', '.', 'She', 'is', 'running', '.', 'Will', 'they', 'stop', 'running', '?', 'I', 'talked', '.', 'She', 'was', 'talking', '.', 'They', 'talked', 'to', 'them', 'about', 'running', '.', 'Who', 'ran', 'to', 'the', 'talking', 'runner', '?', '¡Sebastián', ',', 'Nicolás', ',', 'Alejandro', 'and', 'Jéronimo', 'are', 'going', 'to', 'the', 'store', 'tomorrow', 'morning', '!', 'something', '...', 'is', '!', 'wrong', '(', ')', 'with.', ',', ';', 'this', ':', ':', 'sentence', '.', 'I', 'can', 'not', 'do', 'this', 'anymore', '.', 'I', 'did', 'not', 'know', 'them', '.', 'Why', 'could', 'not', 'you', 'have', 'dinner', 'at', 'the', 'restaurant', '?', 'My', 'favorite', 'movie', 'franchises', ',', 'in', 'order', ':', 'Indiana', 'Jones', ';', 'Marvel', 'Cinematic', 'Universe', ';', 'Star', 'Wars', ';', 'Back', 'to', 'the', 'Future', ';', 'Harry', 'Potter', '.', 'Do', 'not', 'do', 'it', '....', 'Just', 'do', 'not', '.', 'Billy', '!', 'I', 'know', 'what', 'you', 'are', 'doing', '.', 'This', 'is', 'a', 'great', 'little', 'house', 'you', 'have', 'got', 'here', '.', 'John', ':', '``', 'Well', ',', 'well', ',', 'well', '.', \"''\", 'James', ':', '``', 'There', ',', 'there', '.', 'There', ',', 'there', '.', \"''\", 'There', 'are', 'a', 'lot', 'of', 'reasons', 'not', 'to', 'do', 'this', '.', 'There', 'are', '101', 'reasons', 'not', 'to', 'do', 'it', '.', '1000000', 'reasons', ',', 'actually', '.', 'I', 'have', 'to', 'go', 'get', '2', 'tutus', 'from', '2', 'different', 'stores', ',', 'too', '.', '22', '45', '1067', '445', '{', '{', 'Here', 'is', 'some', 'stuff', 'inside', 'of', 'double', 'curly', 'braces', '.', '}', '}', '{', 'Here', 'is', 'more', 'stuff', 'in', 'single', 'curly', 'braces', '.', '}']\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(sample)\n",
    "print(words)  # note that there are noisy words or terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Normalization generally refers to a series of related tasks meant to put all text on a level playing field: converting all text to the same case (upper or lower), removing punctuation, converting numbers to their word equivalents, and so on. Normalization puts all words on equal footing, and allows processing to proceed uniformly.\n",
    "Normalizing text can mean performing a number of tasks, but for our framework we will approach normalization in 3 distinct steps: (1) stemming, (2) lemmatization, and (3) everything else. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'goes', 'bolded', 'text', 'italicized', 'text', 'still', 'run', 'ran', 'running', 'stop', 'running', 'talked', 'talking', 'talked', 'running', 'ran', 'talking', 'runner', 'sebastian', 'nicolas', 'alejandro', 'jeronimo', 'going', 'store', 'tomorrow', 'morning', 'something', 'wrong', 'sentence', 'anymore', 'know', 'could', 'dinner', 'restaurant', 'favorite', 'movie', 'franchises', 'order', 'indiana', 'jones', 'marvel', 'cinematic', 'universe', 'star', 'wars', 'back', 'future', 'harry', 'potter', 'billy', 'know', 'great', 'little', 'house', 'got', 'john', 'well', 'well', 'well', 'james', 'lot', 'reasons', 'one hundred and one', 'reasons', 'one million', 'reasons', 'actually', 'go', 'get', 'two', 'tutus', 'two', 'different', 'stores', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred and forty-five', 'stuff', 'inside', 'double', 'curly', 'braces', 'stuff', 'single', 'curly', 'braces']\n"
     ]
    }
   ],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        # Normalise (normalize) unicode data in Python to remove umlauts, accents etc.\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word) # \\w: an alphanumeric character; \\s: a whitespace character\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)  # 22 -> twenty-two\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "words = normalize(words)\n",
    "\n",
    "print(words)  # better words or terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming and lemming functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed:\n",
      " ['titl', 'goe', 'bold', 'text', 'it', 'text', 'stil', 'run', 'ran', 'run', 'stop', 'run', 'talk', 'talk', 'talk', 'run', 'ran', 'talk', 'run', 'sebast', 'nicola', 'alejandro', 'jeronimo', 'going', 'stor', 'tomorrow', 'morn', 'someth', 'wrong', 'sent', 'anym', 'know', 'could', 'din', 'resta', 'favorit', 'movy', 'franch', 'ord', 'indian', 'jon', 'marvel', 'cinem', 'univers', 'star', 'war', 'back', 'fut', 'harry', 'pot', 'bil', 'know', 'gre', 'littl', 'hous', 'got', 'john', 'wel', 'wel', 'wel', 'jam', 'lot', 'reason', 'one hundred and on', 'reason', 'one million', 'reason', 'act', 'go', 'get', 'two', 'tut', 'two', 'diff', 'stor', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred and forty-five', 'stuff', 'insid', 'doubl', 'cur', 'brac', 'stuff', 'singl', 'cur', 'brac']\n",
      "\n",
      "Lemmatized:\n",
      " ['title', 'go', 'bolded', 'text', 'italicize', 'text', 'still', 'run', 'run', 'run', 'stop', 'run', 'talk', 'talk', 'talk', 'run', 'run', 'talk', 'runner', 'sebastian', 'nicolas', 'alejandro', 'jeronimo', 'go', 'store', 'tomorrow', 'morning', 'something', 'wrong', 'sentence', 'anymore', 'know', 'could', 'dinner', 'restaurant', 'favorite', 'movie', 'franchise', 'order', 'indiana', 'jones', 'marvel', 'cinematic', 'universe', 'star', 'war', 'back', 'future', 'harry', 'potter', 'billy', 'know', 'great', 'little', 'house', 'get', 'john', 'well', 'well', 'well', 'jam', 'lot', 'reason', 'one hundred and one', 'reason', 'one million', 'reason', 'actually', 'go', 'get', 'two', 'tutus', 'two', 'different', 'store', 'twenty-two', 'forty-five', 'one thousand and sixty-seven', 'four hundred and forty-five', 'stuff', 'inside', 'double', 'curly', 'brace', 'stuff', 'single', 'curly', 'brace']\n"
     ]
    }
   ],
   "source": [
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems, lemmas\n",
    "\n",
    "stems, lemmas = stem_and_lemmatize(words)  # use either stemming or lemmatization, but don't use both of them\n",
    "\n",
    "print('Stemmed:\\n', stems)\n",
    "print('\\nLemmatized:\\n', lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Additional Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams\n",
    "The TextBlob.ngrams() method returns a list of tuples of n successive words.\n",
    "[TextBlob](https://textblob.readthedocs.io/en/dev/) is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Now', 'is']),\n",
       " WordList(['is', 'better']),\n",
       " WordList(['better', 'than']),\n",
       " WordList(['than', 'never'])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use TextBlob; $>pip install TextBlob\n",
    "from textblob import TextBlob\n",
    "blob = TextBlob(\"Now is better than never.\")\n",
    "blob.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Now', 'is', 'better']),\n",
       " WordList(['is', 'better', 'than']),\n",
       " WordList(['better', 'than', 'never'])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLTK', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'leading'),\n",
       " ('leading', 'platform'),\n",
       " ('platform', 'for'),\n",
       " ('for', 'building'),\n",
       " ('building', 'Python'),\n",
       " ('Python', 'programs'),\n",
       " ('programs', 'to'),\n",
       " ('to', 'work'),\n",
       " ('work', 'with'),\n",
       " ('with', 'human'),\n",
       " ('human', 'language'),\n",
       " ('language', 'data'),\n",
       " ('data', '.')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use nltk\n",
    "from nltk.util import ngrams, bigrams\n",
    "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "tokens = word_tokenize(input_str)\n",
    "list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLTK', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'leading'),\n",
       " ('leading', 'platform'),\n",
       " ('platform', 'for'),\n",
       " ('for', 'building'),\n",
       " ('building', 'Python'),\n",
       " ('Python', 'programs'),\n",
       " ('programs', 'to'),\n",
       " ('to', 'work'),\n",
       " ('work', 'with'),\n",
       " ('with', 'human'),\n",
       " ('human', 'language'),\n",
       " ('language', 'data'),\n",
       " ('data', '.')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLTK', 'is', 'a'),\n",
       " ('is', 'a', 'leading'),\n",
       " ('a', 'leading', 'platform'),\n",
       " ('leading', 'platform', 'for'),\n",
       " ('platform', 'for', 'building'),\n",
       " ('for', 'building', 'Python'),\n",
       " ('building', 'Python', 'programs'),\n",
       " ('Python', 'programs', 'to'),\n",
       " ('programs', 'to', 'work'),\n",
       " ('to', 'work', 'with'),\n",
       " ('work', 'with', 'human'),\n",
       " ('with', 'human', 'language'),\n",
       " ('human', 'language', 'data'),\n",
       " ('language', 'data', '.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find most common ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('one', 'of', 'the'), 332),\n",
       " (('out', 'of', 'the'), 244),\n",
       " (('of', 'the', 'United'), 235),\n",
       " (('that', 'he', 'was'), 191),\n",
       " (('the', 'United', 'States'), 184),\n",
       " (('that', 'it', 'was'), 180),\n",
       " (('and', 'in', 'the'), 174),\n",
       " (('met', 'with', 'in'), 173),\n",
       " (('up', 'to', 'the'), 159),\n",
       " (('part', 'of', 'the'), 158)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "bigtxt = open('big.txt').read()\n",
    "ngram_counts = Counter(ngrams(bigtxt.split(), 3))\n",
    "ngram_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Received:', 'from'), 12579),\n",
       " (('with', 'ESMTP'), 7188),\n",
       " (('ESMTP', 'id'), 7188),\n",
       " (('Dec', '2007'), 7063),\n",
       " (('Nov', '2007'), 6810),\n",
       " (('-0500', 'Received:'), 5843),\n",
       " (('for', '<source@collab.sakaiproject.org>;'), 5391),\n",
       " (('text/plain;', 'charset=UTF-8'), 5391),\n",
       " (('+0000', '(GMT)'), 4932),\n",
       " (('from', 'murder'), 3594)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigtxt = open('mbox.txt').read()\n",
    "ngram_counts = Counter(ngrams(bigtxt.split(), 2))\n",
    "ngram_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of sentences  =  5\n",
      "[\"this's a sent tokenize test.\", 'this is sent two.', 'is this sent three?', 'sent 4 is cool!', \"Now it's your turn.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"this's a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it's your turn.\"\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "\n",
    "print(\"The length of sentences\", \" = \", len(sent_tokenize_list))\n",
    "print(sent_tokenize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_preprocess(document):\n",
    "    sentences = nltk.sent_tokenize(document) \n",
    "    print(sentences, \"\\n\")\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences] \n",
    "    print(sentences, \"\\n\")\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences] \n",
    "    print(sentences, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = \"\"\"I cannot do this anymore. I did not know them. Why could not you have dinner at the restaurant?\n",
    "\n",
    "My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.\n",
    "\n",
    "do not do it.... Just do not. Billy! I know what you are doing. This is a great little house you have got here.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I cannot do this anymore.', 'I did not know them.', 'Why could not you have dinner at the restaurant?', 'My favorite movie franchises, in order: Indiana Jones; Marvel Cinematic Universe; Star Wars; Back to the Future; Harry Potter.', 'do not do it.... Just do not.', 'Billy!', 'I know what you are doing.', 'This is a great little house you have got here.'] \n",
      "\n",
      "[['I', 'can', 'not', 'do', 'this', 'anymore', '.'], ['I', 'did', 'not', 'know', 'them', '.'], ['Why', 'could', 'not', 'you', 'have', 'dinner', 'at', 'the', 'restaurant', '?'], ['My', 'favorite', 'movie', 'franchises', ',', 'in', 'order', ':', 'Indiana', 'Jones', ';', 'Marvel', 'Cinematic', 'Universe', ';', 'Star', 'Wars', ';', 'Back', 'to', 'the', 'Future', ';', 'Harry', 'Potter', '.'], ['do', 'not', 'do', 'it', '....', 'Just', 'do', 'not', '.'], ['Billy', '!'], ['I', 'know', 'what', 'you', 'are', 'doing', '.'], ['This', 'is', 'a', 'great', 'little', 'house', 'you', 'have', 'got', 'here', '.']] \n",
      "\n",
      "[[('I', 'PRP'), ('can', 'MD'), ('not', 'RB'), ('do', 'VB'), ('this', 'DT'), ('anymore', 'NN'), ('.', '.')], [('I', 'PRP'), ('did', 'VBD'), ('not', 'RB'), ('know', 'VB'), ('them', 'PRP'), ('.', '.')], [('Why', 'WRB'), ('could', 'MD'), ('not', 'RB'), ('you', 'PRP'), ('have', 'VBP'), ('dinner', 'VBN'), ('at', 'IN'), ('the', 'DT'), ('restaurant', 'NN'), ('?', '.')], [('My', 'PRP$'), ('favorite', 'JJ'), ('movie', 'NN'), ('franchises', 'NNS'), (',', ','), ('in', 'IN'), ('order', 'NN'), (':', ':'), ('Indiana', 'NNP'), ('Jones', 'NNP'), (';', ':'), ('Marvel', 'NNP'), ('Cinematic', 'NNP'), ('Universe', 'NNP'), (';', ':'), ('Star', 'NNP'), ('Wars', 'NNP'), (';', ':'), ('Back', 'NNP'), ('to', 'TO'), ('the', 'DT'), ('Future', 'NNP'), (';', ':'), ('Harry', 'NNP'), ('Potter', 'NNP'), ('.', '.')], [('do', 'VB'), ('not', 'RB'), ('do', 'VB'), ('it', 'PRP'), ('....', 'VB'), ('Just', 'NNP'), ('do', 'VB'), ('not', 'RB'), ('.', '.')], [('Billy', 'RB'), ('!', '.')], [('I', 'PRP'), ('know', 'VBP'), ('what', 'WP'), ('you', 'PRP'), ('are', 'VBP'), ('doing', 'VBG'), ('.', '.')], [('This', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('great', 'JJ'), ('little', 'JJ'), ('house', 'NN'), ('you', 'PRP'), ('have', 'VBP'), ('got', 'VBN'), ('here', 'RB'), ('.', '.')]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence_preprocess(sample_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tagging (POS)\n",
    "Part-of-speech tagging aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gaoqiang/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/gaoqiang/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /home/gaoqiang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), (':', ':'), ('an', 'DT'), ('article', 'NN'), (',', ','), ('to', 'TO'), ('write', 'VB'), (',', ','), ('interesting', 'VBG'), (',', ','), ('easily', 'RB'), (',', ','), ('and', 'CC'), (',', ','), ('of', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "### POS TAGGING using nltk ###\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = word_tokenize(\"Parts of speech examples: an article, to write, interesting, easily, and, of\")\n",
    "print(nltk.pos_tag(text)) # input for pos_tag() is a list of words, not a single word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TextBlob](https://textblob.readthedocs.io/en/dev/) is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), ('an', 'DT'), ('article', 'NN'), ('to', 'TO'), ('write', 'VB'), ('interesting', 'VBG'), ('easily', 'RB'), ('and', 'CC'), ('of', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "### POS TAGGING using TextBlob ###\n",
    "input_str=\"Parts of speech examples: an article, to write, interesting, easily, and, of\"\n",
    "from textblob import TextBlob\n",
    "result = TextBlob(input_str)\n",
    "print(result.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking (shallow parsing)\n",
    "Chunking is a natural language process that identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('black', 'JJ'), ('television', 'NN'), ('and', 'CC'), ('a', 'DT'), ('white', 'JJ'), ('stove', 'NN'), ('were', 'VBD'), ('bought', 'VBN'), ('for', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('apartment', 'NN'), ('of', 'IN'), ('John', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# The first step is to determine the part of speech for each word:\n",
    "input_str=\"A black television and a white stove were bought for the new apartment of John.\"\n",
    "result = TextBlob(input_str)\n",
    "print(result.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['black television', 'white stove', 'new apartment', 'john'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract noun phrases using TextBlob\n",
    "# need to install necessary data: $> python -m textblob.download_corpora\n",
    "result.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /home/gaoqiang/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/gaoqiang/nltk_data', '/home/gaoqiang/anaconda3/nltk_data', '/home/gaoqiang/anaconda3/share/nltk_data', '/home/gaoqiang/anaconda3/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
      "[nltk_data] Downloading package words to /home/gaoqiang/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/gaoqiang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gaoqiang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker', quiet=False)\n",
    "# nltk.download('all')\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition\n",
    "Named-entity recognition (NER) aims to find named entities in text and classify them into pre-defined categories (names of persons, locations, organizations, times, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     /home/gaoqiang/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Bill/NNP)\n",
      "  works/VBZ\n",
      "  for/IN\n",
      "  Apple/NNP\n",
      "  so/IN\n",
      "  he/PRP\n",
      "  went/VBD\n",
      "  to/TO\n",
      "  (GPE Boston/NNP)\n",
      "  for/IN\n",
      "  a/DT\n",
      "  conference/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# refer to \"7.5 Named Entity Recognition\" (https://www.nltk.org/book/ch07.html)\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "input_str = \"Bill works for Apple so he went to Boston for a conference.\"\n",
    "print(ne_chunk(pos_tag(word_tokenize(input_str))))   #####need to check why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNet\n",
    "WordNet is a semantically-oriented dictionary of English, similar to a traditional thesaurus but with a richer structure. NLTK includes the English WordNet, with 155,287 words and 117,659 synonym sets. We'll begin by looking at synonyms and how they are accessed in WordNet.\n",
    "Reference: https://www.nltk.org/book/ch02.html and http://www.nltk.org/howto/wordnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('motorcar') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The entity car.n.01 is called a synset, or \"synonym set\", a collection of synonymous words (or \"lemmas\")\n",
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he needs a car to get to work']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('motor_vehicle.n.01')]\n"
     ]
    }
   ],
   "source": [
    "motorcar = wn.synset('car.n.01')\n",
    "types_of_motorcar = motorcar.hypernyms()  # parent synsets\n",
    "print(types_of_motorcar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('stump.n.01'),\n",
       " Synset('crown.n.07'),\n",
       " Synset('burl.n.02'),\n",
       " Synset('trunk.n.01'),\n",
       " Synset('limb.n.02')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('tree.n.01').part_meronyms() # from an item to its components (meronyms) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('demand.n.02.demand')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('supply.n.02.supply').antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Sense Disambiguation\n",
    "\n",
    "Lesk Algorithm (http://www.nltk.org/howto/wsd.html; https://www.nltk.org/_modules/nltk/wsd.html) performs the classic Lesk algorithm for Word Sense Disambiguation (WSD) using the definitions of the ambiguous word. Given an ambiguous word and the context in which the word occurs, Lesk returns a Synset with the highest number of overlapping words between the context sentence and different definitions from each Synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('savings_bank.n.02')\n",
      "Synset('savings_bank.n.02')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "sent = ['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.']\n",
    "print(lesk(sent, 'bank', 'n'))\n",
    "print(lesk(sent, 'bank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bank.n.01') sloping land (especially the slope beside a body of water)\n",
      "Synset('depository_financial_institution.n.01') a financial institution that accepts deposits and channels the money into lending activities\n",
      "Synset('bank.n.03') a long ridge or pile\n",
      "Synset('bank.n.04') an arrangement of similar objects in a row or in tiers\n",
      "Synset('bank.n.05') a supply or stock held in reserve for future use (especially in emergencies)\n",
      "Synset('bank.n.06') the funds held by a gambling house or the dealer in some gambling games\n",
      "Synset('bank.n.07') a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "Synset('savings_bank.n.02') a container (usually with a slot in the top) for keeping money at home\n",
      "Synset('bank.n.09') a building in which the business of banking transacted\n",
      "Synset('bank.n.10') a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
      "Synset('bank.v.01') tip laterally\n",
      "Synset('bank.v.02') enclose with a bank\n",
      "Synset('bank.v.03') do business with a bank or keep an account at a bank\n",
      "Synset('bank.v.04') act as the banker in a game or in gambling\n",
      "Synset('bank.v.05') be in the banking business\n",
      "Synset('deposit.v.02') put into a bank account\n",
      "Synset('bank.v.07') cover with ashes so to control the rate of burning\n",
      "Synset('trust.v.01') have confidence or faith in\n"
     ]
    }
   ],
   "source": [
    "# The definitions for \"bank\" are:\n",
    "# online version: http://wordnetweb.princeton.edu/perl/webwn\n",
    "# \"bank\" in sent is close to the sense of Synset('depository_financial_institution.n.01') & Synset('bank.n.09'). \n",
    "from nltk.corpus import wordnet as wn\n",
    "for ss in wn.synsets('bank'):\n",
    "    print(ss, ss.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('able.s.04')\n",
      "Synset('able.a.01')\n"
     ]
    }
   ],
   "source": [
    "# Test disambiguation of POS tagged \"able\".\n",
    "sent = 'people should be able to marry a person of their choice'.split()\n",
    "print(lesk(sent, 'able'))\n",
    "print(lesk(sent, 'able', pos='a'))  # provide a correct synset\n",
    "# a means ADJECTIVE; s means ADJECTIVE SATELLITE \n",
    "# Certain adjectives bind minimal meaning. e.g. \"dry\", \"good\", etc. Each of these is the center of an adjective synset in WN.\n",
    "# Adjective satellites imposes additional commitments on top of the meaning of the central adjective, \n",
    "# e.g. \"arid\" = \"dry\" + a particular context (i.e. climates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('able.a.01') (usually followed by `to') having the necessary means or skill or know-how or authority to do something\n",
      "Synset('able.s.02') have the skills and qualifications to do things well\n",
      "Synset('able.s.03') having inherent physical or mental ability or capacity\n",
      "Synset('able.s.04') having a strong healthy body\n"
     ]
    }
   ],
   "source": [
    "for ss in wn.synsets('able'):\n",
    "    print(ss, ss.definition())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
