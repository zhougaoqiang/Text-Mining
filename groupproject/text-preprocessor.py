import re
import unicodedata
import spacy
import stanza
import inflect
import contractions
from bs4 import BeautifulSoup
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer, WordNetLemmatizer
from textblob import Word
from enum import Enum


class Tokenizer(Enum):
    NLTK = "nltk"
    SPACY = "spacy"
    STANZA = "stanza"
    REGEX = "regex"
    SPLIT = "split"

# Numberå¤„ç†ç­–ç•¥
class NumberProcessor(Enum):
    NoAction = "no_action"
    ToString = "to_string"
    Remove = "remove"

# Stemmingç­–ç•¥
class Stemmer(Enum):
    NoAction = "no_action"
    Porter = "porter"
    Snowball = "snowball"
    Lancaster = "lancaster"

# Lemmatizationç­–ç•¥
class Lemmatizer(Enum):
    NoAction = "no_action"
    Wordnet = "wordnet"
    Spacy = "spacy"
    Textblob = "textblob"
    Stanza = "stanza"

"""
libraries:
pip install nltk spacy stanza textblob beautifulsoup4 contractions inflect
python -m spacy download en_core_web_sm

if use stanza, make sure executed below command:
import stanza
stanza.download("en")
"""


"""
## **å®Œæ•´çš„å‚æ•°æè¿°ï¼ˆåŒ…å«å¿…ç„¶æ‰§è¡Œçš„æ“ä½œè¯´æ˜ï¼‰**
> âœ… **è¡¨ç¤ºè¯¥æ­¥éª¤æ˜¯å¿…ç„¶æ‰§è¡Œçš„**  
> ğŸ”„ **è¡¨ç¤ºè¯¥æ­¥éª¤å¯ä»¥æ ¹æ®å‚æ•°é…ç½®è¿›è¡Œæ‰§è¡Œæˆ–è·³è¿‡**  

### **1. expand_contractions**
   - **Description:** Expands contractions, e.g., `"can't"` â†’ `"cannot"`.  
   - **Possible values:** `True` (default) / `False`.  
   - **Execution:** ğŸ”„ï¼ˆå¯é€‰ï¼Œå–å†³äºå‚æ•°ï¼‰  

### **2. remove_stopwords**
   - **Description:** Removes stopwords such as `"the"`, `"is"`, `"in"`.  
   - **Possible values:** `True` (default) / `False`.  
   - **Execution:** ğŸ”„ï¼ˆå¯é€‰ï¼Œå–å†³äºå‚æ•°ï¼‰  

### **3. number_action**
   - **Description:** Defines how numbers should be handled.  
   - **Possible values:**  
     - `NumberProcessor.NoAction` (default): Keeps numbers as they are.  
     - `NumberProcessor.ToString`: Converts numbers to words, e.g., `"10"` â†’ `"ten"`.  
     - `NumberProcessor.Remove`: Removes all numbers.  
   - **Execution:** ğŸ”„ï¼ˆå¯é€‰ï¼Œå–å†³äºå‚æ•°ï¼‰  

### **4. stemming**
   - **Description:** Specifies whether stemming should be applied.  
   - **Possible values:**  
     - `Stemmer.NoAction` (default): No stemming applied.  
     - `Stemmer.Porter`: Uses **Porter Stemmer** (suitable for most NLP tasks).  
     - `Stemmer.Snowball`: Uses **Snowball Stemmer** (Porter2, more advanced).  
     - `Stemmer.Lancaster`: Uses **Lancaster Stemmer** (more aggressive and may over-truncate words).  
   - **Execution:** ğŸ”„ï¼ˆå¯é€‰ï¼Œå–å†³äºå‚æ•°ï¼‰  

### **5. lemmatization**
   - **Description:** Defines whether lemmatization should be applied.  
   - **Possible values:**  
     - `Lemmatizer.NoAction` (default): No lemmatization applied.  
     - `Lemmatizer.Wordnet`: Uses **WordNet Lemmatizer** (dictionary-based lemmatization).  
     - `Lemmatizer.Spacy`: Uses **SpaCy Lemmatizer** (more precise, recommended).  
     - `Lemmatizer.Textblob`: Uses **TextBlob Lemmatizer** (simpler but less accurate).  
     - `Lemmatizer.Stanza`: Uses **Stanza Lemmatizer** (supports multiple languages).  
   - **Execution:** ğŸ”„ï¼ˆå¯é€‰ï¼Œå–å†³äºå‚æ•°ï¼‰  
   - **Note:** **If `tokenization=Tokenizer.SPACY` or `tokenization=Tokenizer.STANZA`, lemmatization is automatically applied and does not need to be explicitly executed again.**  

### **6. tokenization**
   - **Description:** Defines how text is split into tokens (words or subwords).  
   - **Possible values:**  
     - `Tokenizer.NLTK` (default): Uses **NLTKâ€™s `word_tokenize()`**. Suitable for general NLP tasks.  
     - `Tokenizer.SPACY`: Uses **SpaCy**. More advanced and includes lemmatization.  
     - `Tokenizer.STANZA`: Uses **Stanza**. Suitable for multilingual text, includes lemmatization.  
     - `Tokenizer.REGEX`: Uses **Regular Expressions** (`re.findall(r'\b\w+\b', text)`). Suitable for quick text preprocessing.  
     - `Tokenizer.SPLIT`: Uses **Pythonâ€™s `.split()`**, simple space-based tokenization.  
   - **Execution:** âœ…ï¼ˆå¿…ç„¶æ‰§è¡Œï¼‰  
   - **Note:** If `Tokenizer.SPACY` or `Tokenizer.STANZA` is used, **Lemmatization is automatically included and will be skipped** in the later steps.  

---

## **å¿…ç„¶æ‰§è¡Œçš„é¢„å¤„ç†æ­¥éª¤**
è¿™äº›æ­¥éª¤ **ä¸å—å‚æ•°æ§åˆ¶ï¼Œå§‹ç»ˆæ‰§è¡Œ**ï¼š
1. **Remove HTML tags** (`__remove_html(text)`) âœ…
2. **Remove brackets** (`__remove_brackets(text)`) âœ…
3. **Convert text to lowercase** (`text.lower()`) âœ…
4. **Tokenization** (`__tokenize(text)`) âœ…
5. **Remove punctuation** (`__remove_punctuation(words)`) âœ…
6. **Remove non-ASCII characters** (`__remove_non_ascii(words)`) âœ…

"""

class TextPreprocessor:
    def __init__(self, expand_contractions=True, remove_stopwords=True,
                 number_action=NumberProcessor.NoAction,
                 stemming=Stemmer.NoAction, lemmatization=Lemmatizer.NoAction,
                 tokenization=Tokenizer.NLTK):

        self.expand_contractions = expand_contractions
        self.tokenization_method = tokenization  # æ–°å¢ tokenization é€‰é¡¹

        if remove_stopwords:
            self.stop_words = set(stopwords.words('english'))
        else:
            self.stop_words = set()

        # è®¾ç½®æ•°å­—å¤„ç†æ–¹å¼
        if number_action == NumberProcessor.ToString:
            self.inflect_engine = inflect.engine()
            self.number_processor = self.__replace_numbers
        elif number_action == NumberProcessor.Remove:
            self.number_processor = self.__remove_numbers
        else:
            self.number_processor = self.__NO_ACTION

        # è®¾ç½® Stemming
        if stemming == Stemmer.Porter:
            self.porter_stemmer = PorterStemmer()
            self.stemmer = self.__stem_with_porter
        elif stemming == Stemmer.Snowball:
            self.snowball_stemmer = SnowballStemmer("english")
            self.stemmer = self.__stem_with_snowball
        elif stemming == Stemmer.Lancaster:
            self.lancaster_stemmer = LancasterStemmer()
            self.stemmer = self.__stem_with_lancaster
        else:
            self.stemmer = self.__NO_ACTION

        # è®¾ç½® Lemmatization
        if lemmatization == Lemmatizer.Wordnet:
            self.wordnet_lemmatizer = WordNetLemmatizer()
            self.lemmatizer = self.__lemmatize_with_wordnet
        elif lemmatization == Lemmatizer.Spacy:
            self.spacy_nlp = spacy.load("en_core_web_sm")
            self.lemmatizer = self.__lemmatize_with_spacy
        elif lemmatization == Lemmatizer.Textblob:
            self.lemmatizer = self.__lemmatize_with_textblob
        elif lemmatization == Lemmatizer.Stanza:
            self.stanza_nlp = stanza.Pipeline(lang="en", processors="tokenize,mwt,pos,lemma")
            self.lemmatizer = self.__lemmatize_with_stanza
        else:
            self.lemmatizer = self.__NO_ACTION

    def process(self, text):
        # Step 1: Remove HTML
        text = self.__remove_html(text)
        # Step 2: Remove brackets
        text = self.__remove_brackets(text)
        # Step 3: Expand contractions
        if self.expand_contractions:
            text = self.__expand_contractions(text)
        # Step 4: Convert to lowercase
        text = text.lower()

        # Step 5: Tokenization
        words = self.__tokenize(text)

        # Step 6: Remove punctuation
        words = self.__remove_punctuation(words)
        # Step 7: Remove non-ASCII characters
        words = self.__remove_non_ascii(words)
        # Step 8: Process numbers
        words = self.number_processor(words)
        # Step 9: Remove stopwords
        words = self.__remove_stopwords(words)
        # Step 10: Apply stemming
        words = self.stemmer(words)
        # **Step 11: ä»…åœ¨é SpaCy æˆ– Stanza é€‰é¡¹ä¸‹è¿›è¡Œ Lemmatization**
        if self.tokenization_method not in [Tokenizer.SPACY, Tokenizer.STANZA]:
            words = self.lemmatizer(words)
        return words

    def __tokenize(self, text):
        """æ ¹æ®é€‰æ‹©çš„ tokenization æ–¹æ³•å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯"""
        if self.tokenization_method == Tokenizer.NLTK:
            return word_tokenize(text)
        elif self.tokenization_method == Tokenizer.SPACY:
            doc = self.spacy_nlp(text)
            return [token.text for token in doc]
        elif self.tokenization_method == Tokenizer.STANZA:
            doc = self.stanza_nlp(text)
            return [word.text for sent in doc.sentences for word in sent.words]
        elif self.tokenization_method == Tokenizer.REGEX:
            return re.findall(r'\b\w+\b', text)
        elif self.tokenization_method == Tokenizer.SPLIT:
            return text.split()
        else:
            raise ValueError("Unsupported tokenization method!")

    def __remove_html(self, text):
        soup = BeautifulSoup(text, "html.parser")
        return soup.get_text()

    def __remove_brackets(self, text):
        return re.sub(r'\[[^]]*\]', '', text)

    def __expand_contractions(self, text):
        return contractions.fix(text)

    def __remove_non_ascii(self, words):
        return [unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8') for word in words]

    def __remove_punctuation(self, words):
        return [re.sub(r'[^\w\s]', '', word) for word in words if word]

    def __remove_stopwords(self, words):
        return [word for word in words if word not in self.stop_words]

    def __replace_numbers(self, words):
        return [self.inflect_engine.number_to_words(word) if word.isdigit() else word for word in words]

    def __remove_numbers(self, words):
        return [word for word in words if not re.fullmatch(r"\d+(\.\d+)?", word)]

    def __NO_ACTION(self, words):
        return words

    # Stemming methods
    def __stem_with_porter(self, words):
        return [self.porter_stemmer.stem(word) for word in words]

    def __stem_with_snowball(self, words):
        return [self.snowball_stemmer.stem(word) for word in words]

    def __stem_with_lancaster(self, words):
        return [self.lancaster_stemmer.stem(word) for word in words]

    # Lemmatization methods
    def __lemmatize_with_wordnet(self, words):
        return [self.wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]

    def __lemmatize_with_spacy(self, words):
        doc = self.spacy_nlp(" ".join(words))
        return [token.lemma_ for token in doc]

    def __lemmatize_with_textblob(self, words):
        return [Word(word).lemmatize() for word in words]

    def __lemmatize_with_stanza(self, words):
        doc = self.stanza_nlp(" ".join(words))
        return [word.lemma for sent in doc.sentences for word in sent.words]
    
"""
preprocessor = TextPreprocessor(
    expand_contractions=True, 
    remove_stopwords=False,  # ä¸å»é™¤åœç”¨è¯ï¼Œä¿ç•™æ›´å¤šä¿¡æ¯
    number_action=NumberProcessor.NoAction,  # ä¿ç•™æ•°å­—
    stemming=Stemmer.NoAction,  # ä¸è¿›è¡Œ Stemming
    lemmatization=Lemmatizer.NoAction,  # ä¸è¿›è¡Œ Lemmatization
    tokenization=Tokenizer.NLTK  # ä½¿ç”¨ NLTK è¿›è¡Œåˆ†è¯
)

preprocessor = TextPreprocessor(
    expand_contractions=True, 
    remove_stopwords=True,  # å»é™¤åœç”¨è¯ï¼Œæé«˜åˆ†ç±»æ¨¡å‹æ€§èƒ½
    number_action=NumberProcessor.Remove,  # ç§»é™¤æ‰€æœ‰æ•°å­—ï¼Œé¿å…å™ªå£°
    stemming=Stemmer.Porter,  # Porter Stemmerï¼Œå‡å°‘å•è¯å˜å½¢
    lemmatization=Lemmatizer.NoAction,  # ä½¿ç”¨ Stemming è€Œé Lemmatization
    tokenization=Tokenizer.NLTK  # ä½¿ç”¨ NLTK è¿›è¡Œåˆ†è¯
)

preprocessor = TextPreprocessor(
    expand_contractions=True, 
    remove_stopwords=True,  # å»é™¤åœç”¨è¯ï¼Œæé«˜åˆ†ç±»æ¨¡å‹æ€§èƒ½
    number_action=NumberProcessor.Remove,  # ç§»é™¤æ‰€æœ‰æ•°å­—ï¼Œé¿å…å™ªå£°
    stemming=Stemmer.Porter,  # Porter Stemmerï¼Œå‡å°‘å•è¯å˜å½¢
    lemmatization=Lemmatizer.NoAction,  # ä½¿ç”¨ Stemming è€Œé Lemmatization
    tokenization=Tokenizer.NLTK  # ä½¿ç”¨ NLTK è¿›è¡Œåˆ†è¯
)

preprocessor = TextPreprocessor(
    expand_contractions=True, 
    remove_stopwords=False,  # è®© Transformer æ¨¡å‹è‡ªè¡Œå­¦ä¹ åœç”¨è¯å½±å“
    number_action=NumberProcessor.NoAction,  # ä¿ç•™åŸå§‹æ•°å­—ä¿¡æ¯
    stemming=Stemmer.NoAction,  # ä¸ä½¿ç”¨ Stemmingï¼Œä¿æŒå•è¯å®Œæ•´æ€§
    lemmatization=Lemmatizer.Spacy,  # SpaCy è¿›è¡Œé«˜è´¨é‡ Lemmatization
    tokenization=Tokenizer.SPACY  # ä½¿ç”¨ SpaCy è¿›è¡Œåˆ†è¯ï¼ˆè‡ªå¸¦ lemmatizationï¼‰
)

preprocessor = TextPreprocessor(
    expand_contractions=True, 
    remove_stopwords=True,  # å»é™¤åœç”¨è¯ï¼Œæé«˜ä¸»é¢˜å»ºæ¨¡è´¨é‡
    number_action=NumberProcessor.Remove,  # ç§»é™¤æ•°å­—ï¼Œé¿å…æ•°å­—å½±å“ä¸»é¢˜
    stemming=Stemmer.NoAction,  # ä¸»é¢˜å»ºæ¨¡é€šå¸¸ä¸éœ€è¦ Stemming
    lemmatization=Lemmatizer.Wordnet,  # ä½¿ç”¨ WordNet è¿›è¡Œ Lemmatization
    tokenization=Tokenizer.NLTK  # ä½¿ç”¨ NLTK è¿›è¡Œåˆ†è¯
)


preprocessor = TextPreprocessor(
    expand_contractions=True, 
    remove_stopwords=False,  # ä¿ç•™åœç”¨è¯ï¼Œä¾å­˜åˆ†æéœ€è¦å®Œæ•´å¥å­ç»“æ„
    number_action=NumberProcessor.NoAction,  # ä¿ç•™åŸå§‹æ•°å­—ä¿¡æ¯
    stemming=Stemmer.NoAction,  # ä¸è¿›è¡Œ Stemmingï¼Œä¿æŒå¥æ³•ç»“æ„
    lemmatization=Lemmatizer.Stanza,  # Stanza è¿›è¡Œé«˜çº§ Lemmatization
    tokenization=Tokenizer.STANZA  # ä½¿ç”¨ Stanza è¿›è¡Œåˆ†è¯ï¼ˆè‡ªå¸¦è¯­æ³•åˆ†æï¼‰
)

"""