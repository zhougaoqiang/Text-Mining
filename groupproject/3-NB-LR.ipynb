{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1597267 entries, 0 to 1597266\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count    Dtype \n",
      "---  ------    --------------    ----- \n",
      " 0   polarity  1597267 non-null  int64 \n",
      " 1   text      1597267 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = \"data/noemoticon_preprocessed.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from textpreprocesshelper import TextPreprocessHelper, NumberProcessor\n",
    "helper  = TextPreprocessHelper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestResult(remove_punctuation = True, number = NumberProcessor.NoAction) :\n",
    "\n",
    "    df[\"tokens\"] = df[\"text\"].apply(word_tokenize)\n",
    "    print(\"tokenize done\")\n",
    "\n",
    "    #after tokenize do extra actions\n",
    "    if remove_punctuation : \n",
    "        df[\"tokens\"] = df[\"tokens\"].apply(helper.remove_punctuation)\n",
    "        \n",
    "    if number == NumberProcessor.Remove :\n",
    "        df[\"tokens\"] = df[\"tokens\"].apply(helper.remove_numbers)\n",
    "    elif number == NumberProcessor.ToString :\n",
    "        df[\"tokens\"] = df[\"tokens\"].apply(helper.replace_numbers)\n",
    "\n",
    "    print(\"additional preprocess done\")\n",
    "\n",
    "     # 训练/测试数据集划分\n",
    "    train_x, test_x, train_y, test_y = train_test_split(\n",
    "        df['tokens'].apply(lambda x: ' '.join(x)),  # word_tokenize 版本\n",
    "        df['polarity'],\n",
    "        random_state=34,\n",
    "        stratify=df['polarity']\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline([ # 构建 Pipeline\n",
    "        ('tfidf', TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}')),  # TF-IDF 向量化\n",
    "        ('clf', MultinomialNB())  # 分类器，默认 Naive Bayes\n",
    "    ])\n",
    "\n",
    "    '''\n",
    "    (1,1) (Unigram)\t['I', 'love', 'NLP']\t基础 NLP 任务，如情感分析\n",
    "    (1,2) (Unigram + Bigram)\t['I', 'love', 'NLP', 'I love', 'love NLP']\t适合上下文敏感的 NLP 任务\n",
    "    (2,2) (Bigram only)\t['I love', 'love NLP']\t更关注短语，但可能信息量不足\n",
    "    (1,3) (Unigram + Bigram + Trigram)\t['I', 'love', 'NLP', 'I love', 'love NLP', 'I love NLP']\t适用于复杂文本建模\n",
    "    '''\n",
    "    # 定义 GridSearch 参数\n",
    "    parameters = {\n",
    "        'tfidf__ngram_range': [(1,1), (1,2)],  # 1-gram 和 1,2-gram\n",
    "        'tfidf__lowercase': [True, False], \n",
    "        'tfidf__max_features': [20000, 50000, 100000],  # 词汇表大小 the value based on check_low_high_frequence_words in 2-preprocessing-analysis.ipynb\n",
    "        'tfidf__stop_words': [None, 'english'],  # 是否使用停用词\n",
    "        'clf': [MultinomialNB()]\n",
    "    }\n",
    "\n",
    "    # 执行 Grid Search\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='accuracy', cv=3)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "\n",
    "    # 打印最佳分数和参数\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "    # 预测测试集\n",
    "    predictions = grid_search.best_estimator_.predict(test_x)\n",
    "\n",
    "    label_mapping = {0: \"Negative\", 4: \"Positive\"}\n",
    "    test_y_labels = test_y.map(label_mapping)\n",
    "    predictions_labels = pd.Series(predictions).map(label_mapping)\n",
    "\n",
    "    # 评估模型性能\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(test_y_labels, predictions_labels))\n",
    "    print(\"Precision:\", metrics.precision_score(test_y_labels, predictions_labels, average='macro'))\n",
    "    print(\"Recall:\", metrics.recall_score(test_y_labels, predictions_labels, average='macro'))\n",
    "    print(\"F1 Score:\", metrics.f1_score(test_y_labels, predictions_labels, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize done\n",
      "additional preprocess done\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best score: 0.801\n",
      "Best parameters set:\n",
      "\tclf: MultinomialNB()\n",
      "\ttfidf__lowercase: True\n",
      "\ttfidf__max_features: 100000\n",
      "\ttfidf__ngram_range: (1, 2)\n",
      "\ttfidf__stop_words: None\n",
      "Accuracy: 0.8023475083705427\n",
      "Precision: 0.8023979475993759\n",
      "Recall: 0.8023484922810957\n",
      "F1 Score: 0.8023396176160206\n"
     ]
    }
   ],
   "source": [
    "getBestResult(remove_punctuation = True, number = NumberProcessor.ToString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize done\n",
      "additional preprocess done\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best score: 0.802\n",
      "Best parameters set:\n",
      "\tclf: MultinomialNB()\n",
      "\ttfidf__lowercase: True\n",
      "\ttfidf__max_features: 100000\n",
      "\ttfidf__ngram_range: (1, 2)\n",
      "\ttfidf__stop_words: None\n",
      "Accuracy: 0.8025478504546513\n",
      "Precision: 0.8026002992250363\n",
      "Recall: 0.8025488534976455\n",
      "F1 Score: 0.8025396521534354\n"
     ]
    }
   ],
   "source": [
    "getBestResult(remove_punctuation = True, number = NumberProcessor.Remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize done\n",
      "additional preprocess done\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best score: 0.801\n",
      "Best parameters set:\n",
      "\tclf: MultinomialNB()\n",
      "\ttfidf__lowercase: True\n",
      "\ttfidf__max_features: 100000\n",
      "\ttfidf__ngram_range: (1, 2)\n",
      "\ttfidf__stop_words: None\n",
      "Accuracy: 0.802440166584443\n",
      "Precision: 0.8024945704322033\n",
      "Recall: 0.8024411883701349\n",
      "F1 Score: 0.8024316483978\n"
     ]
    }
   ],
   "source": [
    "getBestResult(remove_punctuation = True, number = NumberProcessor.NoAction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## no big differences among NoAction, Remove, and to string.\n",
    "\n",
    "## decided to REMOVE to improve generalization, dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize done\n",
      "additional preprocess done\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best score: 0.801\n",
      "Best parameters set:\n",
      "\tclf: MultinomialNB()\n",
      "\ttfidf__lowercase: True\n",
      "\ttfidf__max_features: 100000\n",
      "\ttfidf__ngram_range: (1, 2)\n",
      "\ttfidf__stop_words: None\n",
      "Accuracy: 0.8020194482078148\n",
      "Precision: 0.8020944350267605\n",
      "Recall: 0.8020206490195787\n",
      "F1 Score: 0.8020075924342476\n"
     ]
    }
   ],
   "source": [
    "getBestResult(remove_punctuation = False, number = NumberProcessor.Remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Based on observiation, should remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599999 entries, 0 to 1599998\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count    Dtype \n",
      "---  ------    --------------    ----- \n",
      " 0   polarity  1599999 non-null  int64 \n",
      " 1   text      1599999 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 24.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file_path = \"data/noemoticon_with_header.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "print(df.info())\n",
    "\n",
    "def getLogisticRegressionBestResult(maxIter = 100, vocab_size=100000) :\n",
    "    df[\"tokens\"] = df[\"text\"].apply(word_tokenize)\n",
    "    df[\"tokens\"] = df[\"tokens\"].apply(helper.remove_punctuation)\n",
    "    df[\"tokens\"] = df[\"tokens\"].apply(helper.remove_numbers)\n",
    "\n",
    "     # 训练/测试数据集划分\n",
    "    train_x, test_x, train_y, test_y = train_test_split(\n",
    "        df['tokens'].apply(lambda x: ' '.join(x)),  # word_tokenize 版本\n",
    "        df['polarity'],\n",
    "        random_state=34,\n",
    "        stratify=df['polarity']\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline([ # 构建 Pipeline\n",
    "        ('tfidf', TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}')),  # TF-IDF 向量化\n",
    "        ('clf', LogisticRegression(max_iter= maxIter))  # 分类器，默认 Naive Bayes\n",
    "    ])\n",
    "\n",
    "    # 定义 GridSearch 参数\n",
    "    parameters = {\n",
    "        'tfidf__ngram_range': [(1,2)],  # 1-gram 和 1,2-gram\n",
    "        'tfidf__lowercase': [True], \n",
    "        'tfidf__max_features': [vocab_size],  # 词汇表大小\n",
    "        'tfidf__stop_words': [None],  # 是否使用停用词\n",
    "        'clf__C': [0.1, 1.0, 10.0],\n",
    "        'clf': [LogisticRegression(max_iter= maxIter)]\n",
    "\n",
    "    }\n",
    "\n",
    "    # 执行 Grid Search\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='accuracy', cv=3)\n",
    "    grid_search.fit(train_x, train_y)\n",
    "\n",
    "    # 打印最佳分数和参数\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "    # 预测测试集\n",
    "    predictions = grid_search.best_estimator_.predict(test_x)\n",
    "\n",
    "    label_mapping = {0: \"Negative\", 4: \"Positive\"}\n",
    "    test_y_labels = test_y.map(label_mapping)\n",
    "    predictions_labels = pd.Series(predictions).map(label_mapping)\n",
    "\n",
    "    # 评估模型性能\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(test_y_labels, predictions_labels))\n",
    "    print(\"Precision:\", metrics.precision_score(test_y_labels, predictions_labels, average='macro'))\n",
    "    print(\"Recall:\", metrics.recall_score(test_y_labels, predictions_labels, average='macro'))\n",
    "    print(\"F1 Score:\", metrics.f1_score(test_y_labels, predictions_labels, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best score: 0.820\n",
      "Best parameters set:\n",
      "\tclf: LogisticRegression()\n",
      "\tclf__C: 1.0\n",
      "\ttfidf__lowercase: True\n",
      "\ttfidf__max_features: 100000\n",
      "\ttfidf__ngram_range: (1, 2)\n",
      "\ttfidf__stop_words: None\n",
      "Accuracy: 0.8228325\n",
      "Precision: 0.8229760878128611\n",
      "Recall: 0.8228325\n",
      "F1 Score: 0.822812806652319\n"
     ]
    }
   ],
   "source": [
    "getLogisticRegressionBestResult(maxIter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best score: 0.818\n",
      "Best parameters set:\n",
      "\tclf: LogisticRegression()\n",
      "\tclf__C: 1.0\n",
      "\ttfidf__lowercase: True\n",
      "\ttfidf__max_features: 50000\n",
      "\ttfidf__ngram_range: (1, 2)\n",
      "\ttfidf__stop_words: None\n",
      "Accuracy: 0.8205375\n",
      "Precision: 0.820684429275716\n",
      "Recall: 0.8205374999999999\n",
      "F1 Score: 0.8205169413811533\n"
     ]
    }
   ],
   "source": [
    "getLogisticRegressionBestResult(maxIter=100, vocab_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Best score: 0.816\n",
      "Best parameters set:\n",
      "\tclf: LogisticRegression()\n",
      "\tclf__C: 1.0\n",
      "\ttfidf__lowercase: True\n",
      "\ttfidf__max_features: 30000\n",
      "\ttfidf__ngram_range: (1, 2)\n",
      "\ttfidf__stop_words: None\n",
      "Accuracy: 0.8169775\n",
      "Precision: 0.8171091448886023\n",
      "Recall: 0.8169775\n",
      "F1 Score: 0.8169585030096347\n"
     ]
    }
   ],
   "source": [
    "getLogisticRegressionBestResult(maxIter=100, vocab_size=30000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaoqiang_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
