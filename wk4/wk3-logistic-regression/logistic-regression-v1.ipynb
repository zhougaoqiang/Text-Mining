{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using Logistic Regression (Week 3)\n",
    "\n",
    "This lab is prepared with the tutorial, Deep Learning with PyTorch (https://pytorch.org/tutorials/beginner/nlp/deep_learning_tutorial.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Deep Learning Building Blocks: Affine maps, non-linearities and objectives\n",
    "==========================================================================\n",
    "\n",
    "Deep learning consists of composing linearities with non-linearities in\n",
    "clever ways. The introduction of non-linearities allows for powerful\n",
    "models. In this section, we will play with these core components, make\n",
    "up an objective function, and see how the model is trained.\n",
    "\n",
    "\n",
    "Affine Maps:\n",
    "\n",
    "One of the core workhorses of deep learning is the affine map, which is\n",
    "a function $f(x)$ where\n",
    "\n",
    "\\begin{align}f(x) = Ax + b\\end{align}\n",
    "\n",
    "for a matrix $A$ and vectors $x, b$. The parameters to be\n",
    "learned here are $A$ and $b$. Often, $b$ is refered to\n",
    "as the *bias* term.\n",
    "\n",
    "\n",
    "<b>PyTorch and most other deep learning frameworks do things a little\n",
    "differently than traditional linear algebra. It maps the rows of the\n",
    "input instead of the columns. That is, the $i$'th row of the\n",
    "output below is the mapping of the $i$'th row of the input under\n",
    "$A$, plus the bias term.</b> Look at the example below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7558487d3a50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \n",
      " tensor([[ 0.6614,  0.2669,  0.0617,  0.6213, -0.4519]])\n"
     ]
    }
   ],
   "source": [
    "data = torch.randn(1, 5)  # data is 1x5.\n",
    "print(\"input: \\n\", data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: \n",
      " tensor([[0.0034]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lin = nn.Linear(5, 1)  # maps from R^5 to R^1; Applies a linear transformation to the incoming data with parameters A, b\n",
    "                       # https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "\n",
    "print(\"output: \\n\", lin(data))  # maps from 5 to 1 with parameters A, b; calculate Z score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1633, -0.1743, -0.0326, -0.0403,  0.0648]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin.weight # W weights (initial values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0018], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin.bias # b bias (initial values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-Linearities:\n",
    "\n",
    "First, note the following fact, which will explain why we need\n",
    "non-linearities in the first place. Suppose we have two affine maps\n",
    "$f(x) = Ax + b$ and $g(x) = Cx + d$. What is\n",
    "$f(g(x))$?\n",
    "\n",
    "\\begin{align}f(g(x)) = A(Cx + d) + b = ACx + (Ad + b)\\end{align}\n",
    "\n",
    "$AC$ is a matrix and $Ad + b$ is a vector, so we see that\n",
    "composing affine maps gives you an affine map.\n",
    "\n",
    "From this, you can see that if you wanted your neural network to be long\n",
    "chains of affine compositions, that this adds no new power to your model\n",
    "than just doing a single affine map.\n",
    "\n",
    "If we introduce non-linearities in between the affine layers, this is no\n",
    "longer the case, and we can build much more powerful models.\n",
    "\n",
    "There are a few core non-linearities.\n",
    "$\\tanh(x), \\sigma(x), \\text{ReLU}(x)$ are the most common. You are\n",
    "probably wondering: \"why these functions? I can think of plenty of other\n",
    "non-linearities.\" The reason for this is that they have gradients that\n",
    "are easy to compute, and computing gradients is essential for learning.\n",
    "For example\n",
    "\n",
    "\\begin{align}\\frac{d\\sigma}{dx} = \\sigma(x)(1 - \\sigma(x))\\end{align}\n",
    "\n",
    "A quick note: although you may have learned some neural networks in your\n",
    "intro to AI class where $\\sigma(x)$ was the default non-linearity,\n",
    "typically people shy away from it in practice. This is because the\n",
    "gradient *vanishes* very quickly as the absolute value of the argument\n",
    "grows. Small gradients means it is hard to learn. Most people default to\n",
    "tanh or ReLU.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0034]], grad_fn=<AddmmBackward0>) \n",
      "\n",
      "sigmoid(): \n",
      " tensor([[0.5008]], grad_fn=<SigmoidBackward0>)\n",
      "relu(): \n",
      " tensor([[0.0034]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# In pytorch, most non-linearities are in torch.functional (we have it imported as F)\n",
    "# Note that non-linearites typically don't have parameters like affine maps do.\n",
    "# That is, they don't have weights that are updated during training.\n",
    "data = lin(data)\n",
    "print(data, \"\\n\")\n",
    "print(\"sigmoid(): \\n\", torch.sigmoid(data))\n",
    "print(\"relu(): \\n\", F.relu(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective Functions:\n",
    "\n",
    "The objective function is the function that your network is being\n",
    "trained to minimize (in which case it is often called a *loss function*\n",
    "or *cost function*). This proceeds by first choosing a training\n",
    "instance, running it through your neural network, and then computing the\n",
    "loss of the output. The parameters of the model are then updated by\n",
    "taking the derivative of the loss function. Intuitively, if your model\n",
    "is completely confident in its answer, and its answer is wrong, your\n",
    "loss will be high. If it is very confident in its answer, and its answer\n",
    "is correct, the loss will be low.\n",
    "\n",
    "The idea behind minimizing the loss function on your training examples\n",
    "is that your network will hopefully generalize well and have small loss\n",
    "on unseen examples in your dev set, test set, or in production. An\n",
    "example loss function is the *negative log likelihood loss*, which is a\n",
    "very common objective for multi-class classification. For supervised\n",
    "multi-class classification, this means training the network to minimize\n",
    "the negative log probability of the correct output (or equivalently,\n",
    "maximize the log probability of the correct output).\n",
    "\n",
    "目标函数：\n",
    "\n",
    "目标函数是您的网络正在训练以最小化的函数（在这种情况下，它通常被称为损失函数或成本函数）。这个过程首先选择一个训练实例，将其输入到神经网络中，然后计算输出的损失。接着，模型的参数通过对损失函数求导来进行更新。\n",
    "\n",
    "直观来说，如果您的模型对其答案完全自信，而它的答案是错误的，那么损失会很高。如果它对其答案非常自信，并且答案是正确的，那么损失会很低。\n",
    "\n",
    "最小化训练示例上的损失函数的目的是让您的网络能够很好地泛化，并在开发集、测试集或实际应用中对未见过的示例具有较小的损失。\n",
    "\n",
    "一个示例损失函数是负对数似然损失，这是多类别分类中非常常见的目标函数。在监督式多类别分类中，这意味着训练网络以最小化正确输出的负对数概率（或者等价地，最大化正确输出的对数概率）。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization and Training\n",
    "=========================\n",
    "\n",
    "So what we can compute a loss function for an instance? What do we do\n",
    "with that? We saw earlier that Tensors know how to compute gradients\n",
    "with respect to the things that were used to compute it. Well,\n",
    "since our loss is an Tensor, we can compute gradients with\n",
    "respect to all of the parameters used to compute it! Then we can perform\n",
    "standard gradient updates. Let $\\theta$ be our parameters,\n",
    "$L(\\theta)$ the loss function, and $\\eta$ a positive\n",
    "learning rate. Then:\n",
    "\n",
    "\\begin{align}\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta)\\end{align}\n",
    "\n",
    "There are a huge collection of algorithms and active research in\n",
    "attempting to do something more than just this vanilla gradient update.\n",
    "Many attempt to vary the learning rate based on what is happening at\n",
    "train time. You don't need to worry about what specifically these\n",
    "algorithms are doing unless you are really interested. Torch provides\n",
    "many in the torch.optim package, and they are all completely\n",
    "transparent. Using the simplest gradient update is the same as the more\n",
    "complicated algorithms. Trying different update algorithms and different\n",
    "parameters for the update algorithms (like different initial learning\n",
    "rates) is important in optimizing your network's performance. Often,\n",
    "just replacing vanilla SGD with an optimizer like Adam or RMSProp will\n",
    "boost performance noticably.\n",
    "\n",
    "\n",
    "**优化和训练**\n",
    "\n",
    "那么，我们可以为一个实例计算损失函数，那么接下来该做什么呢？我们之前看到，张量（Tensors）知道如何计算相对于用于计算它的变量的梯度。由于我们的损失是一个张量，我们可以计算相对于所有用于计算它的参数的梯度！然后，我们可以执行标准的梯度更新。设 \\(\\theta\\) 为我们的参数，\\(\\mathcal{L}(\\theta)\\) 为损失函数，\\(\\eta\\) 为一个正的学习率。那么：\n",
    "\n",
    "\\begin{align}\\theta^{(t+1)} = \\theta^{(t)} - \\eta \\nabla_\\theta L(\\theta)\\end{align}\n",
    "\n",
    "有大量的算法和活跃的研究，致力于做比这种基本的梯度更新更多的优化。许多算法尝试根据训练过程中发生的情况调整学习率。除非您真的感兴趣，否则无需担心这些算法具体在做什么。Torch 在 `torch.optim` 包中提供了许多优化算法，并且它们都是完全透明的。使用最简单的梯度更新与更复杂的算法本质上是相同的。尝试不同的更新算法以及不同的更新参数（例如不同的学习率）对优化神经网络的性能非常重要。通常，仅仅用 Adam 或 RMSProp 这样的优化器替换基本的随机梯度下降（SGD），就能显著提升性能。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Network Components in PyTorch\n",
    "======================================\n",
    "\n",
    "Before we move on to our focus on NLP, lets do an annotated example of\n",
    "building a network in PyTorch using only affine maps and\n",
    "non-linearities. We will also see how to compute a loss function, using\n",
    "PyTorch's built in negative log likelihood, and update parameters by\n",
    "backpropagation.\n",
    "\n",
    "All network components should inherit from nn.Module and override the\n",
    "forward() method. That is about it, as far as the boilerplate is\n",
    "concerned. Inheriting from nn.Module provides functionality to your\n",
    "component. For example, it makes it keep track of its trainable\n",
    "parameters, you can swap it between CPU and GPU with the ``.to(device)``\n",
    "method, where device can be a CPU device ``torch.device(\"cpu\")`` or CUDA\n",
    "device ``torch.device(\"cuda:0\")``.\n",
    "\n",
    "Let's write an annotated example of a network that takes in a sparse\n",
    "bag-of-words representation and outputs a probability distribution over\n",
    "two labels: \"English\" and \"Spanish\". This model is just logistic\n",
    "regression.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Logistic Regression Bag-of-Words classifier: \n",
    "\n",
    "Our model will map a sparse BoW representation to log probabilities over\n",
    "labels. We assign each word in the vocab an index. For example, say our\n",
    "entire vocab is two words \"hello\" and \"world\", with indices 0 and 1\n",
    "respectively. The BoW vector for the sentence \"hello hello hello hello\"\n",
    "is\n",
    "\n",
    "\\begin{align}\\left[ 4, 0 \\right]\\end{align}\n",
    "\n",
    "For \"hello world world hello\", it is\n",
    "\n",
    "\\begin{align}\\left[ 2, 2 \\right]\\end{align}\n",
    "\n",
    "etc. In general, it is\n",
    "\n",
    "\\begin{align}\\left[ \\text{Count}(\\text{hello}), \\text{Count}(\\text{world}) \\right]\\end{align}\n",
    "\n",
    "Denote this BOW vector as $x$. The output of our network is:\n",
    "\n",
    "\\begin{align}\\text{Sigmoid}(Ax + b)\\end{align}\n",
    "\n",
    "That is, we pass the input through an affine map and then do sigmoid.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这段内容介绍了**逻辑回归（Logistic Regression）如何应用于袋-of-词（Bag-of-Words, BoW）文本分类**。以下是逐步解析：\n",
    "\n",
    "---\n",
    "\n",
    "### **1. 介绍 BoW 词袋模型**\n",
    "BoW 是一种简单的文本表示方法，它**不考虑词语的顺序**，而是**仅仅统计每个词在文本中出现的次数**。\n",
    "\n",
    "例如：\n",
    "- 词汇表（vocabulary）中只有两个单词：\"hello\" 和 \"world\"。\n",
    "- 我们为这两个单词分配索引：\"hello\" 作为索引 0，\"world\" 作为索引 1。\n",
    "\n",
    "---\n",
    "\n",
    "### **2. 计算 BoW 向量**\n",
    "**示例 1：**\n",
    "对于句子 **\"hello hello hello hello\"**，它的 BoW 表示是：\n",
    "\\[\n",
    "[4, 0]\n",
    "\\]\n",
    "- \"hello\" 出现 4 次，对应索引 0，值为 4。\n",
    "- \"world\" 没出现，对应索引 1，值为 0。\n",
    "\n",
    "**示例 2：**\n",
    "对于句子 **\"hello world world hello\"**，它的 BoW 表示是：\n",
    "\\[\n",
    "[2, 2]\n",
    "\\]\n",
    "- \"hello\" 出现 2 次，对应索引 0，值为 2。\n",
    "- \"world\" 也出现 2 次，对应索引 1，值为 2。\n",
    "\n",
    "**一般公式：**\n",
    "\\[\n",
    "[\\text{Count(hello)}, \\text{Count(world)}]\n",
    "\\]\n",
    "即，BoW 向量的每个元素表示词汇表中相应单词在句子中出现的次数。\n",
    "\n",
    "---\n",
    "\n",
    "### **3. 逻辑回归模型**\n",
    "记 BoW 向量为 \\( x \\)，那么我们的**神经网络输出**为：\n",
    "\\[\n",
    "\\text{Sigmoid}(Ax + b)\n",
    "\\]\n",
    "\n",
    "这里：\n",
    "- \\( A \\) 是一个权重矩阵。\n",
    "- \\( b \\) 是偏置项。\n",
    "- 这意味着，我们首先对输入 \\( x \\) 进行一个**线性变换（仿射变换）**：\n",
    "  \\[\n",
    "  Ax + b\n",
    "  \\]\n",
    "  然后将结果传递到**Sigmoid** 函数中，得到最终的**概率输出**。\n",
    "\n",
    "---\n",
    "\n",
    "### **4. 逻辑回归的作用**\n",
    "- **Sigmoid 函数**将线性变换的结果映射到 \\([0,1]\\)，可以解释为**属于某个类别的概率**。\n",
    "- **如果输出接近 1**，意味着模型认为该文本更可能属于某个类别（比如 \"positive sentiment\"）。\n",
    "- **如果输出接近 0**，意味着该文本更可能属于另一个类别（比如 \"negative sentiment\"）。\n",
    "\n",
    "---\n",
    "\n",
    "### **5. 结论**\n",
    "- **BoW 仅统计词频**，不考虑单词顺序，因此是一个**简单但有效**的文本表示方式。\n",
    "- **逻辑回归**在 BoW 上应用，通过学习权重 \\( A \\) 和偏置 \\( b \\) 来进行分类。\n",
    "- **Sigmoid 函数**用于输出概率，从而进行二元分类（如情感分析）。\n",
    "\n",
    "如果你有更具体的疑问，欢迎继续讨论！ 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n"
     ]
    }
   ],
   "source": [
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
    "# index into the Bag of words vector\n",
    "#这个词典可以用于构建 Bag-of-Words（BoW）表示，即将每个句子转换为一个向量，该向量的每个索引对应一个单词的出现次数。\n",
    "word_to_ix = {}\n",
    "for sent, _ in data + test_data: #_（下划线）是一个占位符变量，表示我们不会使用这个值。\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))    # shape: [26] ; vector\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "\n",
    "    #.view(1, -1) 将 vec 变成 1 x N 形状的矩阵（行向量），方便神经网络处理：\n",
    "    return vec.view(1, -1)    # return a matrix: 1 x len(vec)\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    #将文本标签（如 \"SPANISH\" 或 \"ENGLISH\"）转换为 数值标签（0 或 1）\n",
    "    return torch.LongTensor([label_to_ix[label]])  # [0] or [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([1, 26])\n"
     ]
    }
   ],
   "source": [
    "bow_vector = make_bow_vector(\"me gusta comer en la cafeteria\".split(), word_to_ix)\n",
    "print(bow_vector)\n",
    "print(bow_vector.shape) # matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "target = make_target(\"SPANISH\", label_to_ix)\n",
    "print(target)  \n",
    "print(target.shape) # vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.]])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(target.float().unsqueeze(dim=0)) # matrix  ; convert size([1]) to size([1,1])\n",
    "print(target.float().unsqueeze(dim=0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.]])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(target.float().view(1,-1)) # matrix ; convert size([1]) to size([1,1])\n",
    "print(target.float().view(1,-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1714,  0.0610, -0.0730, -0.1184, -0.0329, -0.0846, -0.0628,  0.0094,\n",
      "          0.1169,  0.1066, -0.1917,  0.1216,  0.0548,  0.1860,  0.1294, -0.1787,\n",
      "         -0.1865, -0.0946,  0.1722, -0.0327,  0.0839, -0.0911,  0.1924, -0.0830,\n",
      "          0.1471,  0.0023]], requires_grad=True) ; shape:  torch.Size([1, 26])\n",
      "Parameter containing:\n",
      "tensor([-0.1033], requires_grad=True) ; shape:  torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(word_to_ix)   # 26 in this example\n",
    "OUTPUT_SIZE = 1\n",
    "\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, vocab_size, output_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Make sure you understand why the input dimension is vocab_size\n",
    "        # and the output is output_size!\n",
    "        \n",
    "        self.linear = nn.Linear(vocab_size, output_size)  # output_size = 1\n",
    "\n",
    "        # NOTE! The non-linearity sigmoid does not have parameters! So we don't need\n",
    "        # to worry about that here\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through sigmoid.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        \n",
    "        return torch.sigmoid(self.linear(bow_vec))  # use sigmoid\n",
    "\n",
    "model = BoWClassifier(VOCAB_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "# the model knows its parameters.  The first output below is A, the second is b.\n",
    "# Whenever you assign a component to a class variable in the __init__ function\n",
    "# of a module, which was done with the line\n",
    "# self.linear = nn.Linear(...)\n",
    "# Then through some Python magic from the PyTorch devs, your module\n",
    "# (in this case, BoWClassifier) will store knowledge of the nn.Linear's parameters\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param, \"; shape: \", param.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4552]])\n"
     ]
    }
   ],
   "source": [
    "# To run the model, pass in a BoW vector\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    sample = data[0]                                      # ([me, gusta, comer, en, la, cafeteria], \"SPANISH\")\n",
    "    bow_vector = make_bow_vector(sample[0], word_to_ix)  \n",
    "    output_probs = model(bow_vector)\n",
    "    print(output_probs)   # i.e. probablity of English class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4727]])\n",
      "tensor([[0.4507]])\n",
      "tensor([-0.1917], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Run on test data before we train, just to see a before-and-after\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:                   # first example:  ([Yo, creo, que, si], \"SPANISH\")  \n",
    "                                                        # second example: ([it, is, lost, on, me], \"ENGLISH\")\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        output_probs = model(bow_vec)\n",
    "        print(output_probs)\n",
    "\n",
    "# Print the matrix column corresponding to \"creo\"\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets train! To do this, we pass instances through to get \n",
    "probabilities, compute a loss function, compute the gradient of the loss\n",
    "function, and then update the parameters with a gradient step. Loss\n",
    "functions are provided by Torch in the nn package. <b>nn.BCELoss() is the\n",
    "Binary Cross Entropy between the target and the output.</b> It also defines optimization\n",
    "functions in torch.optim. Here, we will just use SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yo', 'creo', 'que', 'si'] :  0.14345665276050568 : SPANISH\n",
      "['it', 'is', 'lost', 'on', 'me'] :  0.8789470195770264 : ENGLISH\n",
      "tensor([-0.9463], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.BCELoss()   # use BCELoss() to measures the Binary Cross Entropy between the target and the output\n",
    "                               # use CrossEntropyLoss for multi-class classification, such as 0, 1, 2, or 3\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Usually you want to pass over the training data several times.\n",
    "# 100 is much bigger than on a real data set, but real datasets have more than\n",
    "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        # Step 1. Remember that PyTorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Make our BOW vector and also we must wrap the target in a\n",
    "        # Tensor as an integer. For example, if the target is SPANISH, then\n",
    "        # we wrap the integer 0. \n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        target = make_target(label, label_to_ix).float().view(1,-1) # make type and dimension (1x1) right                                                      \n",
    "        #print(target)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        output_probs = model(bow_vec)\n",
    "        #print(output_probs)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss = loss_function(output_probs, target)   # target has the same shape as the output_probs\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:  # first example:  ([Yo, creo, que, si], \"SPANISH\")  \n",
    "                                       # second example: ([it, is, lost, on, me], \"ENGLISH\")\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        output_probs = model(bow_vec)\n",
    "        if output_probs.item() < 0.5:\n",
    "            print(instance, \": \", output_probs.item(), \": SPANISH\")\n",
    "        else:\n",
    "            print(instance, \": \", output_probs.item(), \": ENGLISH\")\n",
    "\n",
    "# Weight for \"creo\" goes down!\n",
    "print(next(model.parameters())[:, word_to_ix[\"creo\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the right answer! You can see that the probability for English is lower than 0.5 in the first example, and the probability for English is much higher than 0.5 in the second for the test data, as it should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me :  0.31328997015953064\n",
      "gusta :  -0.6709371209144592\n",
      "comer :  -0.805001437664032\n",
      "en :  -0.8504132032394409\n",
      "la :  -0.764838695526123\n",
      "cafeteria :  -0.8165594339370728\n",
      "Give :  0.8109734654426575\n",
      "it :  1.327536940574646\n",
      "to :  1.4350569248199463\n",
      "No :  -0.20364010334014893\n",
      "creo :  -0.946277916431427\n",
      "que :  -0.632987916469574\n",
      "sea :  -0.25544866919517517\n",
      "una :  -0.5685372352600098\n",
      "buena :  -0.6251159906387329\n",
      "idea :  -0.4889252483844757\n",
      "is :  0.25785887241363525\n",
      "not :  0.3497346043586731\n",
      "a :  0.6165398359298706\n",
      "good :  0.4116608798503876\n",
      "get :  0.5282576084136963\n",
      "lost :  0.3531903028488159\n",
      "at :  0.6367605924606323\n",
      "Yo :  -0.08297643065452576\n",
      "si :  0.14707234501838684\n",
      "on :  0.002322339452803135\n",
      "\n",
      "b:  -0.2717020809650421\n"
     ]
    }
   ],
   "source": [
    "for i, param in enumerate(model.parameters()):\n",
    "    if i == 0:\n",
    "        for key, value in word_to_ix.items():\n",
    "            print(key, \": \", param[0][value].item())\n",
    "    else:\n",
    "        print(\"\\nb: \", param.item()) # Use torch.Tensor.item() to get a Python number from a tensor containing a single value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise:\n",
    "\n",
    "1. Build a simple sentiment classifier with a new data given below, and predict test_data: utilize BoWClassifier() defined in previous steps.\n",
    "1. 使用下面给出的新数据构建一个简单的情绪分类器，并预测测试数据：利用前面步骤中定义的 BoWClassifier()。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"well done\".split(), \"positive\"),\n",
    "        (\"good work\".split(), \"positive\"),\n",
    "        (\"great effort\".split(), \"positive\"),\n",
    "        (\"weak\".split(), \"negative\"),\n",
    "        (\"poor effort\".split(), \"negative\"),\n",
    "        (\"not good\".split(), \"negative\")]\n",
    "\n",
    "test_data = [(\"great work\".split(), \"positive\"),\n",
    "             (\"good job\".split(), \"positive\"),\n",
    "             (\"poor work\".split(), \"negative\"),\n",
    "             (\"not great\".split(), \"negative\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'well': 0, 'done': 1, 'good': 2, 'work': 3, 'great': 4, 'effort': 5, 'weak': 6, 'poor': 7, 'not': 8, 'job': 9}\n"
     ]
    }
   ],
   "source": [
    "# word_to_ix maps each word in the vocab to a unique integer, which will be its\n",
    "# index into the Bag of words vector\n",
    "word_to_ix = {}\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "print(word_to_ix)\n",
    "\n",
    "label_to_ix = {\"negative\": 0, \"positive\": 1}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
