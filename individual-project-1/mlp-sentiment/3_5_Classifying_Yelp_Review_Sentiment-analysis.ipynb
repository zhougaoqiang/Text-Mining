{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Yelp Reviews (Classifying Sentiment of Restaurant Reviews, chapter 3) \n",
    "\n",
    "### Source: Chapter 3, Natural Language Processing with Pytorch. (2019). Delip Rao and Brian McMahan. O’Reilly: source code available on https://github.com/joosthub/PyTorchNLPBook\n",
    "\n",
    "### PyTorch tutorial: refer to https://pytorch.org/tutorials/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\\nuse to prevent kernal crash due to re-initializing  libiomp5md.dll\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "use to prevent kernal crash due to re-initializing  libiomp5md.dll\n",
    "'''\n",
    "# import os\n",
    "# os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Vectorization classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx         # _token_to_idx: {'<UNK>':0, 'apple':1, 'banana':2, ....., 'zzz':10000}\n",
    "                                                  # _idx_to_token: {0:'<UNK>', 1:'apple', 2:'banana', ....., 10000:'zzz'}\n",
    "\n",
    "        self._idx_to_token = {idx: token          \n",
    "                              for token, idx in self._token_to_idx.items()} \n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "              \n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:                   # if unk_token is defined, unknown words are handled\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Vectorizer_Type(Enum):\n",
    "    One_Hot = \"one-hot\"\n",
    "    Term_Freq = \"term-freq\"\n",
    "    TF_IDF = \"tf-idf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''Followings are preprocessing functions'''\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "from nltk.corpus import opinion_lexicon\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('opinion_lexicon')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()  ###better for Sentiment Analysis\n",
    "\n",
    "def remove_special_char(text) :\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "\n",
    "def case_folding(text) :\n",
    "    return text.lower()\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_stop_words(text) :\n",
    "    tokens = word_tokenize(text)\n",
    "    return  \" \".join([word for word in tokens if word not in stop_words])\n",
    "\n",
    "def wordnet_lemmatizer(text) : \n",
    "    ''' tokens = word_tokenize(text) should not tokenize again, directly split '''\n",
    "    tokens = text.split()\n",
    "    return  \" \".join( [lemmatizer.lemmatize(word) for word in tokens ])\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
    "    def __init__(self, review_vocab, rating_vocab, vectorizer_type, tfidf_vectorizer=None, enable_opinion_lexicon=False, use_word_embedding=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_vocab (Vocabulary): maps words to integers\n",
    "            rating_vocab (Vocabulary): maps class labels to integers; {'negative':0, 'positive':1}\n",
    "        \"\"\"\n",
    "        self.review_vocab = review_vocab     # {'<UNK>':0, 'apple':1, 'banana':2, ....., 'zzz':10000}\n",
    "        self.rating_vocab = rating_vocab     # {'negative':0, 'positive':1}\n",
    "\n",
    "        ####newly added\n",
    "        self.vectorize_type = vectorizer_type\n",
    "        self.tfidf_vectorizer = tfidf_vectorizer\n",
    "        self.enable_opinion_lexicon = enable_opinion_lexicon\n",
    "        self.use_word_embedding = use_word_embedding\n",
    "\n",
    "        if self.enable_opinion_lexicon :\n",
    "            self.positive_words = set(opinion_lexicon.positive())\n",
    "            self.negative_words = set(opinion_lexicon.negative())\n",
    "        \n",
    "        if self.use_word_embedding:\n",
    "            self.glove_vectors = api.load(\"glove-wiki-gigaword-300\")\n",
    "            self.embedding_dim = 300\n",
    "            print(\"GloVe model loaded!\")\n",
    "\n",
    "    def vectorize(self, review):\n",
    "        \"\"\"Create a collapsed one-hot vector for the review\n",
    "        \n",
    "        Args:\n",
    "            review (str): the review \n",
    "        Returns:\n",
    "            one_hot (np.ndarray): the collapsed one-hot encoding   \n",
    "        \"\"\"         \n",
    "        if self.vectorize_type == Vectorizer_Type.TF_IDF:  # if use TF-IDF, directly return\n",
    "            text_vector = self.tfidf_vectorizer.transform([review]).toarray().flatten() # i.e. [0.85, 0.47, ......, 0.0, 0.0] \n",
    "        elif self.vectorize_type == Vectorizer_Type.Term_Freq: ##if enable term_frequency\n",
    "            text_vector = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "            for token in review.split(\" \"):\n",
    "                # if token not in string.punctuation: ####will handle by preprocessing\n",
    "                    index = self.review_vocab.lookup_token(token)\n",
    "                    text_vector[index] += 1\n",
    "        else:  # One-Hot Encoding\n",
    "            text_vector = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "            for token in review.split(\" \"):\n",
    "                # if token not in string.punctuation:  ####will handle by preprocessing\n",
    "                    text_vector[self.review_vocab.lookup_token(token)] = 1  # E.g., \"Unfortunately, the frustration of being Dr. Go...\" -> [0, 0, 1, 0, 1, ....., 0, 0]\n",
    "\n",
    "        '''\n",
    "        Word Embedding(GloVe)\n",
    "        In sentiment analysis, glove is more suitable than word2vec.\n",
    "\n",
    "        If the word embedding is flattened directly, the dimensions will be too large. if the length of words in sentence is 100, the dims will be 100 * 300 = 30000, cannot control the dims.\n",
    "        hence, I decide to use word embedding as additional dims instead of acting directly on vectorization, which means not parallel to IF-IDF, one-hot and term-freq.\n",
    "        after deciede to act as additional dims, the calculation use mean instead of others (such as max, weighting), because this is sentiment classification, the overall feeling may be more important\n",
    "        '''\n",
    "        if self.use_word_embedding:\n",
    "            embedding_vector = self.word_embedding_vector(review)\n",
    "            text_vector = np.concatenate((text_vector, embedding_vector))  # concatenate vector #add 300 dims\n",
    "\n",
    "        '''\n",
    "        To maintain the independence of different features to avoid information loss, \n",
    "        hence, decided to add two more dimensions instead of feature reweighting\n",
    "        '''\n",
    "        if self.enable_opinion_lexicon :  \n",
    "            lexicon_features = self.sentiment_lexicon_features(review)\n",
    "            combined_vector = np.concatenate((text_vector, lexicon_features)) #add 2 dims\n",
    "            return combined_vector\n",
    "        \n",
    "        return text_vector\n",
    "    \n",
    "    '''for opinion lexicon'''\n",
    "    def sentiment_lexicon_features(self, review):\n",
    "        tokens = review.split()\n",
    "        pos_count = sum(1 for word in tokens if word in self.positive_words)\n",
    "        neg_count = sum(1 for word in tokens if word in self.negative_words)\n",
    "        return np.array([pos_count, neg_count], dtype=np.float32)\n",
    "    \n",
    "    '''for word embedding'''\n",
    "    def word_embedding_vector(self, review):\n",
    "        tokens = review.split()\n",
    "        word_vectors = []\n",
    "        \n",
    "        for word in tokens:\n",
    "            if word in self.glove_vectors:\n",
    "                word_vectors.append(self.glove_vectors[word])  # get word gloVe vector\n",
    "        \n",
    "        if len(word_vectors) == 0:\n",
    "            return np.zeros(self.embedding_dim, dtype=np.float32)  # if not find, return 0\n",
    "        else:\n",
    "            return np.mean(word_vectors, axis=0)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, vectorizer_type, enable_opinion_lexicon, use_word_embedding, ngram_option, cutoff=25):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            cls: class name, i.e. ReviewVectorizer\n",
    "            review_df (pandas.DataFrame): the review dataset\n",
    "            cutoff (int): the parameter for frequency-based filtering\n",
    "        Returns:\n",
    "            an instance of the ReviewVectorizer\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary(add_unk=True)   # create an instance of Vocabulary class\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        # Add ratings\n",
    "        for rating in sorted(set(review_df.rating)):  \n",
    "            rating_vocab.add_token(rating)            # {'negative':0, 'positive':1}\n",
    "\n",
    "        # Add top words if count > provided count\n",
    "        word_counts = Counter()\n",
    "        for review in review_df.review:\n",
    "            for word in review.split(\" \"):\n",
    "                # if word not in string.punctuation:\n",
    "                word_counts[word] += 1\n",
    "               \n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)          # {'<UNK>':0, 'apple':1, 'banana':2, ....., 'zzz':10000}\n",
    "\n",
    "        if vectorizer_type == Vectorizer_Type.TF_IDF :\n",
    "            tfidf_vectorizer = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=ngram_option)  # support unigram + bigram words\n",
    "            tfidf_vectorizer.fit(review_df.review)\n",
    "        else :\n",
    "            tfidf_vectorizer = None\n",
    "\n",
    "        return cls(review_vocab, rating_vocab, vectorizer_type, tfidf_vectorizer, enable_opinion_lexicon, use_word_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作用：\n",
    "\n",
    "负责 将文本转换为数值向量，用于机器学习模型训练。\n",
    "支持 词频 (Term Frequency) 统计 或 One-hot 编码。\n",
    "该类主要负责 文本向量化，将文本转换为模型可以使用的 数值表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (ReviewVectorizer): vectorizer instantiated from dataset\n",
    "        \"\"\"\n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.review_df[self.review_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.review_df[self.review_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.review_df[self.review_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv, preprocessing_args, vectorizer_type, enable_opinion_lexicon, use_word_embedding, ngram_option, frequency_cutoff):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            cls: class name, i.e. ReviewDataset\n",
    "            review_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        # train_review_df = review_df[review_df.split=='train']\n",
    "        train_review_df = review_df[review_df.split=='train'].copy() ### avoid too many alerts\n",
    "\n",
    "        ##### APPLY PREPROCESSING HERE\n",
    "        text_col = \"review\"\n",
    "        if preprocessing_args.remove_special_char :\n",
    "            train_review_df[text_col] = train_review_df[text_col].apply(remove_special_char)\n",
    "\n",
    "        if preprocessing_args.case_folding :\n",
    "            train_review_df[text_col] = train_review_df[text_col].apply(case_folding)\n",
    "\n",
    "        if preprocessing_args.remove_punctuation :\n",
    "            train_review_df[text_col] = train_review_df[text_col].apply(remove_punctuation)\n",
    "\n",
    "        if preprocessing_args.expand_contractions :\n",
    "            train_review_df[text_col] = train_review_df[text_col].apply(expand_contractions)\n",
    "        \n",
    "        if preprocessing_args.remove_stop_words :\n",
    "            train_review_df[text_col] = train_review_df[text_col].apply(remove_stop_words)\n",
    "\n",
    "        if preprocessing_args.wordnet_lemmatizer :\n",
    "            train_review_df[text_col] = train_review_df[text_col].apply(wordnet_lemmatizer)\n",
    "\n",
    "\n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(train_review_df, vectorizer_type, enable_opinion_lexicon, use_word_embedding, ngram_option, frequency_cutoff))\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \n",
    "        \n",
    "        Args:\n",
    "            split (str): one of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        review_vector = \\\n",
    "            self._vectorizer.vectorize(row.review)\n",
    "\n",
    "        rating_index = \\\n",
    "            self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "\n",
    "        return {'x_data': review_vector,           #  e.g., { 'x_data': [0, 0, 1, 0, 1, ....., 0, 0],\n",
    "                'y_target': rating_index}          #          'y_target': 0  }\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size  # the floor division // rounds the result down to the nearest whole number\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    # drop_last: set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. \n",
    "    # If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False)\n",
    "    # need to use sampler option for balanced data: \n",
    "    # https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "    \n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():  # name: x_data & y_target\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: ReviewClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    \"\"\" a simple perceptron based classifier \"\"\"\n",
    "    def __init__(self, num_features, hidden_dim, enable_second_hidden_layer, dropout_rate, enable_batch_norm):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int): the size of the input feature vector\n",
    "            hidden_dim   (int): the size of hidden dimension\n",
    "        \"\"\"\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.enable_second_hidden_layer = enable_second_hidden_layer\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.enable_batch_norm = enable_batch_norm\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=num_features, out_features=hidden_dim)\n",
    "        if enable_batch_norm:\n",
    "            self.batch_norm1 = nn.BatchNorm1d(hidden_dim)\n",
    "        if self.dropout_rate != 0:\n",
    "            self.dropout1 = nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "        if enable_second_hidden_layer :\n",
    "            self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2) ###base on experience from lab assignment 1\n",
    "            if enable_batch_norm:\n",
    "                self.batch_norm2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "            if self.dropout_rate != 0 :    \n",
    "                self.dropout2 = nn.Dropout(p=self.dropout_rate)\n",
    "            self.fc_final = nn.Linear(hidden_dim // 2, out_features=1)\n",
    "        else :\n",
    "            self.fc_final = nn.Linear(hidden_dim, out_features=1)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        \"\"\"The forward pass of the classifier    \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be [batch, num_features]\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be [batch]\n",
    "        \"\"\"\n",
    "        intermediate = self.fc1(x_in)            # [batch, num_features] -> [batch, hidden_dim]\n",
    "        if self.enable_batch_norm:\n",
    "            intermediate = self.batch_norm1(intermediate)  \n",
    "        intermediate = F.relu(intermediate)      # [batch, hidden_dim]\n",
    "        if self.dropout_rate != 0 :\n",
    "            intermediate = self.dropout1(intermediate)\n",
    "\n",
    "        if self.enable_second_hidden_layer:\n",
    "            # Second hidden layer\n",
    "            intermediate = self.fc2(intermediate)\n",
    "            if self.enable_batch_norm:\n",
    "                intermediate = self.batch_norm2(intermediate)\n",
    "            intermediate = F.relu(intermediate)\n",
    "            if self.dropout_rate != 0 :\n",
    "                intermediate = self.dropout2(intermediate)\n",
    "\n",
    "        y_out = self.fc_final(intermediate)           # [batch, hidden_dim] -> [batch, out_features]\n",
    "        return torch.sigmoid(y_out).squeeze()    # [batch, 1] -> [batch] (e.g., [0.3, 0.1, 0.7, 0.8, ..., 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t  # update 'early_stopping_best_val'\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        # print(train_state['train_acc'])\n",
    "        if train_state['early_stopping_step'] >= args.early_stopping_criteria or train_state['train_acc'][-1] > 99.9 : ### if  train_state['train_acc'] > 0.999 also can stop directly\n",
    "            train_state['stop_early'] = True\n",
    "        else : \n",
    "            train_state['stop_early'] = False\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (y_pred>0.5).cpu().long()\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()  # item() to get a Python number from a tensor containing a single value\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and some prep work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/ch3/yelp/model.pth\n",
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "current parameters are only original value.\n",
    "'''\n",
    "preprocessing_args = Namespace(\n",
    "    remove_punctuation=False,\n",
    "    remove_special_char=False,\n",
    "    case_folding=False,\n",
    "    expand_contractions=False,\n",
    "    remove_stop_words=False,\n",
    "    wordnet_lemmatizer=False,\n",
    ")\n",
    "\n",
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file='model.pth',\n",
    "    review_csv='data/yelp/reviews_with_splits_lite.csv',\n",
    "    save_dir='model_storage/ch3/yelp/',\n",
    "    # No Model hyper parameters\n",
    "    hidden_dim=20,\n",
    "    # Training hyper parameters\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=3,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    # Runtime options\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    "\n",
    "\n",
    "    ###Newly added\n",
    "    ######for vectorizer\n",
    "    vectorizer_type=Vectorizer_Type.One_Hot,\n",
    "    ngram_option=(1,1),\n",
    "    enable_opinion_lexicon=False,\n",
    "    use_word_embedding=False,\n",
    "\n",
    "    ####for ReviewClassifier\n",
    "    enable_second_hidden_layer=False,\n",
    "    dropout_rate=0,\n",
    "    enable_batch_norm=False,\n",
    "\n",
    "    ####for optimizer\n",
    "    weight_decay=0, ##change to 0 to disable\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs ; creat dirs if they don't exist\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(preprocessing_args, args) :\n",
    "    # print(\"Loading dataset and creating vectorizer\")\n",
    "    # create dataset and vectorizer\n",
    "    dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv, preprocessing_args, \n",
    "                                                            vectorizer_type=args.vectorizer_type,\n",
    "                                                            enable_opinion_lexicon=args.enable_opinion_lexicon,\n",
    "                                                            use_word_embedding=args.use_word_embedding,\n",
    "                                                            ngram_option=args.ngram_option,\n",
    "                                                            frequency_cutoff=args.frequency_cutoff)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    if args.vectorizer_type == Vectorizer_Type.TF_IDF :\n",
    "        num_features = len(vectorizer.tfidf_vectorizer.vocabulary_)\n",
    "        if args.enable_opinion_lexicon :\n",
    "            num_features = num_features + 2\n",
    "        if args.use_word_embedding :\n",
    "            num_features = num_features + 300\n",
    "            print(f'number of features => {num_features}')\n",
    "        classifier = ReviewClassifier(num_features=num_features, hidden_dim=args.hidden_dim, enable_second_hidden_layer=args.enable_second_hidden_layer, dropout_rate=args.dropout_rate, enable_batch_norm=args.enable_batch_norm)\n",
    "    else :\n",
    "        num_features=len(vectorizer.review_vocab) ### original code\n",
    "        if args.enable_opinion_lexicon :\n",
    "            num_features = num_features + 2\n",
    "        if args.use_word_embedding :\n",
    "            num_features = num_features + 300\n",
    "        print(f'number of features => {num_features}')\n",
    "        classifier = ReviewClassifier(num_features=num_features, hidden_dim=args.hidden_dim, enable_second_hidden_layer=args.enable_second_hidden_layer, dropout_rate=args.dropout_rate, enable_batch_norm=args.enable_batch_norm)\n",
    "    return dataset, vectorizer, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str(vectorizer.review_vocab), str(vectorizer.rating_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "1. lr_scheduler.ReduceLROnPlateau(): Reduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This scheduler reads a metrics quantity and if no improvement is seen for a ‘patience’ number of epochs, the learning rate is reduced.(https://pytorch.org/docs/stable/optim.html)\n",
    "> - mode (str) – One of min, max. In min mode, lr will be reduced when the quantity monitored has stopped decreasing; in max mode it will be reduced when the quantity monitored has stopped increasing. Default: ‘min’.<br>\n",
    ">- factor (float) – Factor by which the learning rate will be reduced. new_lr = lr * factor. Default: 0.1.<br>\n",
    ">- patience (int) – Number of epochs with no improvement after which learning rate will be reduced. For example, if patience = 2, then we will ignore the first 2 epochs with no improvement, and will only decrease the LR after the 3rd epoch if the loss still hasn’t improved then. Default: 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     30
    ]
   },
   "outputs": [],
   "source": [
    "def train_model(preprocessing_args, args, dataset, classifier, disable_tqdm=False) :\n",
    "    classifier = classifier.to(args.device)\n",
    "\n",
    "    loss_func = nn.BCELoss()\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                    mode='min', factor=0.1,\n",
    "                                                    patience=10) # Reduce learning rate when a metric has stopped improving.\n",
    "\n",
    "    train_state = make_train_state(args)\n",
    "\n",
    "    epoch_bar = tqdm(desc='training routine', total=args.num_epochs, position=0, disable=disable_tqdm)  # progress bar\n",
    "\n",
    "    dataset.set_split('train')\n",
    "    train_bar = tqdm(desc='split=train', total=dataset.get_num_batches(args.batch_size), position=1, leave=True, disable=disable_tqdm)\n",
    "\n",
    "    dataset.set_split('val')\n",
    "    val_bar = tqdm(desc='split=val', total=dataset.get_num_batches(args.batch_size), position=1, leave=True, disable=disable_tqdm)\n",
    "\n",
    "    try:\n",
    "        for epoch_index in range(args.num_epochs):\n",
    "            train_state['epoch_index'] = epoch_index\n",
    "\n",
    "            # Iterate over training dataset\n",
    "\n",
    "            # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "            dataset.set_split('train')\n",
    "            batch_generator = generate_batches(dataset, \n",
    "                                            batch_size=args.batch_size, \n",
    "                                            device=args.device)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            classifier.train()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                # the training routine is these 5 steps:\n",
    "\n",
    "                # --------------------------------------\n",
    "                # step 1. zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # step 2. compute the output\n",
    "                y_pred = classifier(x_in=batch_dict['x_data'].float())  # [batch, num_features] -> [batch]\n",
    "\n",
    "                # step 3. compute the loss\n",
    "                loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # step 4. use loss to produce gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # step 5. use optimizer to take gradient step\n",
    "                optimizer.step()\n",
    "                # -----------------------------------------\n",
    "                # compute the accuracy\n",
    "                acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "                \n",
    "                # update bar\n",
    "                train_bar.set_postfix(loss=running_loss, \n",
    "                                    acc=running_acc, \n",
    "                                    epoch=epoch_index)\n",
    "                train_bar.update()\n",
    "            \n",
    "            train_state['train_loss'].append(running_loss)  # train_loss for each epoch\n",
    "            train_state['train_acc'].append(running_acc)    # train_acc for each epoch\n",
    "            \n",
    "            # Iterate over val dataset\n",
    "\n",
    "            # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "            dataset.set_split('val')\n",
    "            batch_generator = generate_batches(dataset, \n",
    "                                            batch_size=args.batch_size, \n",
    "                                            device=args.device)\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "            classifier.eval()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "                # compute the output\n",
    "                y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "                # step 3. compute the loss\n",
    "                loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # compute the accuracy\n",
    "                acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "                \n",
    "                val_bar.set_postfix(loss=running_loss, \n",
    "                                    acc=running_acc, \n",
    "                                    epoch=epoch_index)\n",
    "                val_bar.update()\n",
    "\n",
    "            train_state['val_loss'].append(running_loss)  # val_loss for each epoch\n",
    "            train_state['val_acc'].append(running_acc)    # val_acc for each epoch\n",
    "\n",
    "            train_state = update_train_state(args=args, model=classifier,\n",
    "                                            train_state=train_state)\n",
    "\n",
    "            scheduler.step(train_state['val_loss'][-1])  # adjust learning rate\n",
    "\n",
    "            if train_state['stop_early']:\n",
    "                break\n",
    "\n",
    "            train_bar.n = 0   # reset number of finished iterations\n",
    "            val_bar.n = 0\n",
    "            epoch_bar.update()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting loop\")\n",
    "    return train_state, loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_loss_and_acc(train_state, title) :\n",
    "    epochs = range(1, len(train_state['train_acc']) + 1)\n",
    "    fig, ax1 = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "    fig.suptitle(title, fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    ax1[0].plot(epochs, train_state['train_loss'], 'bo-', label='Training Loss')\n",
    "    ax1[0].plot(epochs, train_state['val_loss'], 'r-', label='Validation Loss') \n",
    "    ax1[0].set_title('Training and Validation Loss')\n",
    "    ax1[0].set_xlabel('Epochs')\n",
    "    ax1[0].set_ylabel('Loss')\n",
    "    ax1[0].legend()\n",
    "\n",
    "    ax1[1].plot(epochs, train_state['train_acc'], 'go-', label='Training Accuracy')\n",
    "    ax1[1].plot(epochs, train_state['val_acc'], 'm-', label='Validation Accuracy') \n",
    "    ax1[1].set_title('Training and Validation Accuracy')\n",
    "    ax1[1].set_xlabel('Epochs')\n",
    "    ax1[1].set_ylabel('Accuracy')\n",
    "    ax1[1].legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.tight_layout() \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "def check_test_data(preprocessing_args, args, dataset, classifier, train_state, loss_func, enable_print_info=True) :\n",
    "    classifier.load_state_dict(torch.load(train_state['model_filename'], weights_only=False))\n",
    "    classifier = classifier.to(args.device)\n",
    "\n",
    "    dataset.set_split('test')\n",
    "    batch_generator = generate_batches(dataset, \n",
    "                                    batch_size=args.batch_size, \n",
    "                                    device=args.device)\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "    y_pred_list = []    # store predicted values for confusion matrix\n",
    "    y_target_list = []  # ground truth value\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # compute the output\n",
    "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "        \n",
    "        # store predicted values and ground truth values for calculating confusion matrix\n",
    "        y_pred_list.extend((y_pred>0.5).cpu().long().numpy())\n",
    "        y_target_list.extend(batch_dict['y_target'].cpu().numpy())\n",
    "        \n",
    "        # compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "    train_state['test_loss'] = running_loss\n",
    "    train_state['test_acc'] = running_acc\n",
    "\n",
    "    if enable_print_info: \n",
    "        print(\"==================================dividing line==================================\")\n",
    "        print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
    "        print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))\n",
    "\n",
    "    if enable_print_info: \n",
    "        rating_classes = []\n",
    "        for i in range(len(dataset._vectorizer.rating_vocab)):\n",
    "            rating_classes.append(dataset._vectorizer.rating_vocab.lookup_index(i))\n",
    "        # print(\"==================================dividing line=======================\")\n",
    "        # print(rating_classes)\n",
    "\n",
    "    if enable_print_info: \n",
    "        cm = confusion_matrix(y_target_list, y_pred_list)\n",
    "        cm_df = pd.DataFrame(cm.T, index=rating_classes, columns=rating_classes)\n",
    "        cm_df.index.name = 'Predicted'\n",
    "        cm_df.columns.name = 'True'\n",
    "        print(\"==================================dividing line==================================\")\n",
    "        print(cm_df)\n",
    "\n",
    "        print(\"==================================dividing line==================================\")\n",
    "        print(classification_report(y_target_list, y_pred_list))\n",
    "\n",
    "    return train_state['test_acc'], train_state['test_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### orignial result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features => 2264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1d356e62d343b9854ab91c6142ab75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa220c7bb774002a04571b3a7614ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4765dffa2b164f6d92ed474937ef8499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================dividing line==================================\n",
      "Test loss: 0.352\n",
      "Test Accuracy: 86.52\n",
      "==================================dividing line==================================\n",
      "True       negative  positive\n",
      "Predicted                    \n",
      "negative        454        80\n",
      "positive         58       432\n",
      "==================================dividing line==================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87       512\n",
      "           1       0.88      0.84      0.86       512\n",
      "\n",
      "    accuracy                           0.87      1024\n",
      "   macro avg       0.87      0.87      0.87      1024\n",
      "weighted avg       0.87      0.87      0.87      1024\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(86.5234375, 0.3516443111002445)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, vectorizer, classifier = init(preprocessing_args, args)\n",
    "train_state, loss_func = train_model(preprocessing_args, args, dataset=dataset, classifier=classifier, disable_tqdm=False)\n",
    "# draw_loss_and_acc(train_state, \"ORIGINAL\")\n",
    "check_test_data(preprocessing_args, args, dataset, classifier, train_state, loss_func, enable_print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/ch3/yelp/model.pth\n",
      "Using CUDA: True\n",
      "number of features => 2264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21485c1e704f437db538a99fc4979b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8291a976ae014652a8404fe7aa56a23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "262f83d16f5045d3aeb875b0b8d5766a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.71875\n",
      "0.3512718714773655\n"
     ]
    }
   ],
   "source": [
    "preprocessing_args = Namespace(\n",
    "    remove_punctuation=False, #True\n",
    "    remove_special_char=False, #True\n",
    "    case_folding=False, #True\n",
    "    expand_contractions=False, #True\n",
    "    remove_stop_words=False, #True\n",
    "    wordnet_lemmatizer=False, #True\n",
    ")\n",
    "\n",
    "args = Namespace(\n",
    "    frequency_cutoff=25, #0, 50\n",
    "    model_state_file='model.pth',\n",
    "    review_csv='data/yelp/reviews_with_splits_lite.csv',\n",
    "    save_dir='model_storage/ch3/yelp/',\n",
    "    hidden_dim=20, #10, 30\n",
    "    batch_size=128, #64, 256\n",
    "    early_stopping_criteria=3,\n",
    "    learning_rate=0.001, #0.0001, 0.01\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False, \n",
    "    vectorizer_type=Vectorizer_Type.One_Hot, #Vectorizer_Type.Term_Freq,  Vectorizer_Type.TF_IDF, One_Hot\n",
    "    ngram_option=(1,1), #(1,2) only apply for Vectorizer_Type.TF_IDF\n",
    "    enable_opinion_lexicon=False, #True\n",
    "    use_word_embedding=False,  #True\n",
    "    enable_second_hidden_layer=False,  #True\n",
    "    dropout_rate=0,  #0.25, 0.5\n",
    "    enable_batch_norm=False,  #True\n",
    "    weight_decay=0.0001, #0.0001?\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs ; creat dirs if they don't exist\n",
    "handle_dirs(args.save_dir)\n",
    "\n",
    "# Train model\n",
    "dataset, vectorizer, classifier = init(preprocessing_args, args)\n",
    "train_state, loss_func = train_model(preprocessing_args, args, dataset=dataset, classifier=classifier)\n",
    "# Compute accuracy\n",
    "test_acc, test_loss = check_test_data(preprocessing_args, args, dataset, classifier, train_state, loss_func, enable_print_info=False)\n",
    "print(test_acc)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paramters selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nimport optuna\\nimport pandas as pd\\n\\nvectorizer_type_mapping = {\\n    \"one-hot\": Vectorizer_Type.One_Hot,\\n    \"term-freq\": Vectorizer_Type.Term_Freq,\\n    \"tf-idf\": Vectorizer_Type.TF_IDF\\n}\\n\\ndef objective(trial):\\n\\n    preprocessing_args = Namespace(\\n        remove_special_char=trial.suggest_categorical(\"remove_special_char\", [True, False]),\\n        case_folding=trial.suggest_categorical(\"case_folding\", [True, False]),\\n        remove_punctuation=trial.suggest_categorical(\"remove_punctuation\", [True, False]),\\n        expand_contractions=trial.suggest_categorical(\"expand_contractions\", [True, False]),\\n        remove_stop_words=trial.suggest_categorical(\"remove_stop_words\", [True, False]),\\n        wordnet_lemmatizer=trial.suggest_categorical(\"wordnet_lemmatizer\", [True, False]))\\n\\n    vectorizer_type_str = trial.suggest_categorical(\"vectorizer_type\", [\"one-hot\", \"term-freq\", \"tf-idf\"])\\n\\n    args = Namespace(\\n        model_state_file=\\'model.pth\\',\\n        review_csv=\\'data/yelp/reviews_with_splits_lite.csv\\',\\n        save_dir=\\'model_storage/ch3/yelp/\\',\\n        early_stopping_criteria=5,\\n        num_epochs=100,\\n        seed=1337,\\n        catch_keyboard_interrupt=True,\\n        cuda=True,\\n        expand_filepaths_to_save_dir=True,\\n        reload_from_files=False,\\n\\n        frequency_cutoff=trial.suggest_categorical(\"frequency_cutoff\", [0, 25]),\\n        learning_rate=trial.suggest_categorical(\"learning_rate\", [0.0001, 0.0005, 0.001, 0.01]),\\n        hidden_dim=trial.suggest_categorical(\"hidden_dim\", [10, 20,50,100]),\\n        batch_size=trial.suggest_categorical(\"batch_size\", [64, 128, 256]), ###i think this no need to check\\n        # vectorizer_type=trial.suggest_categorical(\"vectorizer_type\", [Vectorizer_Type.One_Hot, Vectorizer_Type.TF_IDF, Vectorizer_Type.Term_Freq]),\\n         vectorizer_type=vectorizer_type_mapping[vectorizer_type_str],  ## prevent too much warning\\n        enable_opinion_lexicon=trial.suggest_categorical(\"enable_opinion_lexicon\", [True, False]),\\n        use_word_embedding=trial.suggest_categorical(\"use_word_embedding\", [True, False]),\\n        enable_second_hidden_layer=trial.suggest_categorical(\"enable_second_hidden_layer\", [True, False]),\\n        enable_drop_out=trial.suggest_categorical(\"enable_drop_out\", [True, False]),\\n        enable_batch_norm=trial.suggest_categorical(\"enable_batch_norm\", [True, False]),\\n        weight_decay=trial.suggest_categorical(\"weight_decay\", [0, 0.0001, 0.001, 0.01]),\\n    )\\n\\n    # if args.expand_filepaths_to_save_dir:\\n    #     args.model_state_file = os.path.join(args.save_dir,\\n    #                                         args.model_state_file)    \\n    #     print(\"Expanded filepaths: \")\\n    #     print(\"\\t{}\".format(args.model_state_file))\\n    if not torch.cuda.is_available():\\n        args.cuda = False\\n    # print(\"Using CUDA: {}\".format(args.cuda))\\n\\n    args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\\n    # Set seed for reproducibility\\n    set_seed_everywhere(args.seed, args.cuda)\\n    # handle dirs ; creat dirs if they don\\'t exist\\n    handle_dirs(args.save_dir)\\n\\n    # Train model\\n    dataset, vectorizer, classifier = init(preprocessing_args, args)\\n    train_state, loss_func = train_model(preprocessing_args, args, dataset=dataset, classifier=classifier)\\n    # Compute accuracy\\n    test_acc = check_test_data(preprocessing_args, args, dataset, classifier, train_state, loss_func, enable_print_info=False)\\n    return test_acc  # Optuna will maximize this value\\n\\nstudy = optuna.create_study(direction=\"maximize\")\\nstudy.optimize(objective, n_trials=100)\\n\\ndf = study.trials_dataframe().sort_values(\"value\", ascending=False).head(10)\\n\\n# Extract top 10 best trials with all parameters\\ntop_10_trials = df[[\\n    \"value\",  # Test Accuracy\\n\\n    # 🔹 Preprocessing Parameters\\n    \"params_remove_special_char\", \"params_case_folding\", \"params_remove_punctuation\",\\n    \"params_expand_contractions\", \"params_remove_stop_words\", \"params_wordnet_lemmatizer\",\\n\\n    # 🔹 Model Hyperparameters\\n    \"params_frequency_cutoff\", \"params_learning_rate\", \"params_hidden_dim\",\\n    \"params_batch_size\", \"params_vectorizer_type\", \"params_enable_opinion_lexicon\",\\n    \"params_use_word_embedding\", \"params_enable_second_hidden_layer\",\\n    \"params_enable_drop_out\", \"params_enable_batch_norm\", \"params_weight_decay\"\\n]].to_dict(orient=\"records\")\\n\\n# Print the top 5 best parameter combinations\\nprint(\"\\nTop 10 Best Hyperparameter Combinations:\")\\nfor i, trial in enumerate(top_10_trials):\\n    print(f\"\\n🔹 **Rank {i+1}:** Accuracy = {trial[\\'value\\']:.4f}\")\\n    print({key.replace(\"params_\", \"\"): value for key, value in trial.items() if key != \"value\"})\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "vectorizer_type_mapping = {\n",
    "    \"one-hot\": Vectorizer_Type.One_Hot,\n",
    "    \"term-freq\": Vectorizer_Type.Term_Freq,\n",
    "    \"tf-idf\": Vectorizer_Type.TF_IDF\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    preprocessing_args = Namespace(\n",
    "        remove_special_char=trial.suggest_categorical(\"remove_special_char\", [True, False]),\n",
    "        case_folding=trial.suggest_categorical(\"case_folding\", [True, False]),\n",
    "        remove_punctuation=trial.suggest_categorical(\"remove_punctuation\", [True, False]),\n",
    "        expand_contractions=trial.suggest_categorical(\"expand_contractions\", [True, False]),\n",
    "        remove_stop_words=trial.suggest_categorical(\"remove_stop_words\", [True, False]),\n",
    "        wordnet_lemmatizer=trial.suggest_categorical(\"wordnet_lemmatizer\", [True, False]))\n",
    "\n",
    "    vectorizer_type_str = trial.suggest_categorical(\"vectorizer_type\", [\"one-hot\", \"term-freq\", \"tf-idf\"])\n",
    "\n",
    "    args = Namespace(\n",
    "        model_state_file='model.pth',\n",
    "        review_csv='data/yelp/reviews_with_splits_lite.csv',\n",
    "        save_dir='model_storage/ch3/yelp/',\n",
    "        early_stopping_criteria=5,\n",
    "        num_epochs=100,\n",
    "        seed=1337,\n",
    "        catch_keyboard_interrupt=True,\n",
    "        cuda=True,\n",
    "        expand_filepaths_to_save_dir=True,\n",
    "        reload_from_files=False,\n",
    "\n",
    "        frequency_cutoff=trial.suggest_categorical(\"frequency_cutoff\", [0, 25]),\n",
    "        learning_rate=trial.suggest_categorical(\"learning_rate\", [0.0001, 0.0005, 0.001, 0.01]),\n",
    "        hidden_dim=trial.suggest_categorical(\"hidden_dim\", [10, 20,50,100]),\n",
    "        batch_size=trial.suggest_categorical(\"batch_size\", [64, 128, 256]), ###i think this no need to check\n",
    "        # vectorizer_type=trial.suggest_categorical(\"vectorizer_type\", [Vectorizer_Type.One_Hot, Vectorizer_Type.TF_IDF, Vectorizer_Type.Term_Freq]),\n",
    "         vectorizer_type=vectorizer_type_mapping[vectorizer_type_str],  ## prevent too much warning\n",
    "        enable_opinion_lexicon=trial.suggest_categorical(\"enable_opinion_lexicon\", [True, False]),\n",
    "        use_word_embedding=trial.suggest_categorical(\"use_word_embedding\", [True, False]),\n",
    "        enable_second_hidden_layer=trial.suggest_categorical(\"enable_second_hidden_layer\", [True, False]),\n",
    "        enable_drop_out=trial.suggest_categorical(\"enable_drop_out\", [True, False]),\n",
    "        enable_batch_norm=trial.suggest_categorical(\"enable_batch_norm\", [True, False]),\n",
    "        weight_decay=trial.suggest_categorical(\"weight_decay\", [0, 0.0001, 0.001, 0.01]),\n",
    "    )\n",
    "\n",
    "    # if args.expand_filepaths_to_save_dir:\n",
    "    #     args.model_state_file = os.path.join(args.save_dir,\n",
    "    #                                         args.model_state_file)    \n",
    "    #     print(\"Expanded filepaths: \")\n",
    "    #     print(\"\\t{}\".format(args.model_state_file))\n",
    "    if not torch.cuda.is_available():\n",
    "        args.cuda = False\n",
    "    # print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "    args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    # Set seed for reproducibility\n",
    "    set_seed_everywhere(args.seed, args.cuda)\n",
    "    # handle dirs ; creat dirs if they don't exist\n",
    "    handle_dirs(args.save_dir)\n",
    "\n",
    "    # Train model\n",
    "    dataset, vectorizer, classifier = init(preprocessing_args, args)\n",
    "    train_state, loss_func = train_model(preprocessing_args, args, dataset=dataset, classifier=classifier)\n",
    "    # Compute accuracy\n",
    "    test_acc = check_test_data(preprocessing_args, args, dataset, classifier, train_state, loss_func, enable_print_info=False)\n",
    "    return test_acc  # Optuna will maximize this value\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "df = study.trials_dataframe().sort_values(\"value\", ascending=False).head(10)\n",
    "\n",
    "# Extract top 10 best trials with all parameters\n",
    "top_10_trials = df[[\n",
    "    \"value\",  # Test Accuracy\n",
    "\n",
    "    # 🔹 Preprocessing Parameters\n",
    "    \"params_remove_special_char\", \"params_case_folding\", \"params_remove_punctuation\",\n",
    "    \"params_expand_contractions\", \"params_remove_stop_words\", \"params_wordnet_lemmatizer\",\n",
    "\n",
    "    # 🔹 Model Hyperparameters\n",
    "    \"params_frequency_cutoff\", \"params_learning_rate\", \"params_hidden_dim\",\n",
    "    \"params_batch_size\", \"params_vectorizer_type\", \"params_enable_opinion_lexicon\",\n",
    "    \"params_use_word_embedding\", \"params_enable_second_hidden_layer\",\n",
    "    \"params_enable_drop_out\", \"params_enable_batch_norm\", \"params_weight_decay\"\n",
    "]].to_dict(orient=\"records\")\n",
    "\n",
    "# Print the top 5 best parameter combinations\n",
    "print(\"\\nTop 10 Best Hyperparameter Combinations:\")\n",
    "for i, trial in enumerate(top_10_trials):\n",
    "    print(f\"\\n🔹 **Rank {i+1}:** Accuracy = {trial['value']:.4f}\")\n",
    "    print({key.replace(\"params_\", \"\"): value for key, value in trial.items() if key != \"value\"})\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "156px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "5",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
