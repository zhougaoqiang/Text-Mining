{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Yelp Reviews (Classifying Sentiment of Restaurant Reviews, chapter 3) \n",
    "\n",
    "### Source: Chapter 3, Natural Language Processing with Pytorch. (2019). Delip Rao and Brian McMahan. O’Reilly: source code available on https://github.com/joosthub/PyTorchNLPBook\n",
    "\n",
    "### PyTorch tutorial: refer to https://pytorch.org/tutorials/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport os\\nos.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\\nuse to prevent kernal crash due to re-initializing  libiomp5md.dll\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "use to prevent kernal crash due to re-initializing  libiomp5md.dll\n",
    "'''\n",
    "# import os\n",
    "# os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Vectorization classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx         # _token_to_idx: {'<UNK>':0, 'apple':1, 'banana':2, ....., 'zzz':10000}\n",
    "                                                  # _idx_to_token: {0:'<UNK>', 1:'apple', 2:'banana', ....., 10000:'zzz'}\n",
    "\n",
    "        self._idx_to_token = {idx: token          \n",
    "                              for token, idx in self._token_to_idx.items()} \n",
    "        \n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        \n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token) \n",
    "              \n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:                   # if unk_token is defined, unknown words are handled\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        \n",
    "        Args: \n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class Vectorizer_Type(Enum):\n",
    "    One_Hot = \"one-hot\"\n",
    "    Term_Freq = \"term-freq\"\n",
    "    TF_IDF = \"tf-idf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Zhou&Wan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Zhou&Wan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Zhou&Wan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     C:\\Users\\Zhou&Wan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''Followings are preprocessing functions'''\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "from nltk.corpus import opinion_lexicon\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('opinion_lexicon')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()  ###better for Sentiment Analysis\n",
    "\n",
    "def remove_special_char(text) :\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "\n",
    "def case_folding(text) :\n",
    "    return text.lower()\n",
    "\n",
    "def expand_contractions(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_stop_words(text) :\n",
    "    tokens = word_tokenize(text)\n",
    "    return  \" \".join([word for word in tokens if word not in stop_words])\n",
    "\n",
    "def wordnet_lemmatizer(text) : \n",
    "    ''' tokens = word_tokenize(text) should not tokenize again, directly split '''\n",
    "    tokens = text.split()\n",
    "    return  \" \".join( [lemmatizer.lemmatize(word) for word in tokens ])\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
    "    def __init__(self, review_vocab, rating_vocab, vectorizer_type, tfidf_vectorizer=None, enable_opinion_lexicon=False, use_word_embedding=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_vocab (Vocabulary): maps words to integers\n",
    "            rating_vocab (Vocabulary): maps class labels to integers; {'negative':0, 'positive':1}\n",
    "        \"\"\"\n",
    "        self.review_vocab = review_vocab     # {'<UNK>':0, 'apple':1, 'banana':2, ....., 'zzz':10000}\n",
    "        self.rating_vocab = rating_vocab     # {'negative':0, 'positive':1}\n",
    "\n",
    "        ####newly added\n",
    "        self.vectorize_type = vectorizer_type\n",
    "        self.tfidf_vectorizer = tfidf_vectorizer\n",
    "        self.enable_opinion_lexicon = enable_opinion_lexicon\n",
    "        self.use_word_embedding = use_word_embedding\n",
    "\n",
    "        if self.enable_opinion_lexicon :\n",
    "            self.positive_words = set(opinion_lexicon.positive())\n",
    "            self.negative_words = set(opinion_lexicon.negative())\n",
    "        \n",
    "        if self.use_word_embedding:\n",
    "            self.glove_vectors = api.load(\"glove-wiki-gigaword-300\")\n",
    "            self.embedding_dim = 300\n",
    "            print(\"GloVe model loaded!\")\n",
    "\n",
    "    def vectorize(self, review):\n",
    "        \"\"\"Create a collapsed one-hot vector for the review\n",
    "        \n",
    "        Args:\n",
    "            review (str): the review \n",
    "        Returns:\n",
    "            one_hot (np.ndarray): the collapsed one-hot encoding   \n",
    "        \"\"\"         \n",
    "        if self.vectorize_type == Vectorizer_Type.TF_IDF:  # if use TF-IDF, directly return\n",
    "            text_vector = self.tfidf_vectorizer.transform([review]).toarray().flatten() # i.e. [0.85, 0.47, ......, 0.0, 0.0] \n",
    "        elif self.vectorize_type == Vectorizer_Type.Term_Freq: ##if enable term_frequency\n",
    "            text_vector = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "            for token in review.split(\" \"):\n",
    "                # if token not in string.punctuation: ####will handle by preprocessing\n",
    "                    index = self.review_vocab.lookup_token(token)\n",
    "                    text_vector[index] += 1\n",
    "        else:  # One-Hot Encoding\n",
    "            text_vector = np.zeros(len(self.review_vocab), dtype=np.float32)\n",
    "            for token in review.split(\" \"):\n",
    "                # if token not in string.punctuation:  ####will handle by preprocessing\n",
    "                    text_vector[self.review_vocab.lookup_token(token)] = 1  # E.g., \"Unfortunately, the frustration of being Dr. Go...\" -> [0, 0, 1, 0, 1, ....., 0, 0]\n",
    "\n",
    "        '''\n",
    "        Word Embedding(GloVe)\n",
    "        In sentiment analysis, glove is more suitable than word2vec.\n",
    "\n",
    "        If the word embedding is flattened directly, the dimensions will be too large. if the length of words in sentence is 100, the dims will be 100 * 300 = 30000, cannot control the dims.\n",
    "        hence, I decide to use word embedding as additional dims instead of acting directly on vectorization, which means not parallel to IF-IDF, one-hot and term-freq.\n",
    "        after deciede to act as additional dims, the calculation use mean instead of others (such as max, weighting), because this is sentiment classification, the overall feeling may be more important\n",
    "        '''\n",
    "        if self.use_word_embedding:\n",
    "            embedding_vector = self.word_embedding_vector(review)\n",
    "            text_vector = np.concatenate((text_vector, embedding_vector))  # concatenate vector #add 300 dims\n",
    "\n",
    "        '''\n",
    "        To maintain the independence of different features to avoid information loss, \n",
    "        hence, decided to add two more dimensions instead of feature reweighting\n",
    "        '''\n",
    "        if self.enable_opinion_lexicon :  \n",
    "            lexicon_features = self.sentiment_lexicon_features(review)\n",
    "            combined_vector = np.concatenate((text_vector, lexicon_features)) #add 2 dims\n",
    "            return combined_vector\n",
    "        \n",
    "        return text_vector\n",
    "    \n",
    "    '''for opinion lexicon'''\n",
    "    def sentiment_lexicon_features(self, review):\n",
    "        tokens = review.split()\n",
    "        pos_count = sum(1 for word in tokens if word in self.positive_words)\n",
    "        neg_count = sum(1 for word in tokens if word in self.negative_words)\n",
    "        return np.array([pos_count, neg_count], dtype=np.float32)\n",
    "    \n",
    "    '''for word embedding'''\n",
    "    def word_embedding_vector(self, review):\n",
    "        tokens = review.split()\n",
    "        word_vectors = []\n",
    "        \n",
    "        for word in tokens:\n",
    "            if word in self.glove_vectors:\n",
    "                word_vectors.append(self.glove_vectors[word])  # get word gloVe vector\n",
    "        \n",
    "        if len(word_vectors) == 0:\n",
    "            return np.zeros(self.embedding_dim, dtype=np.float32)  # if not find, return 0\n",
    "        else:\n",
    "            return np.mean(word_vectors, axis=0)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, review_df, vectorizer_type, enable_opinion_lexicon, use_word_embedding, ngram_option, cutoff=25):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        \n",
    "        Args:\n",
    "            cls: class name, i.e. ReviewVectorizer\n",
    "            review_df (pandas.DataFrame): the review dataset\n",
    "            cutoff (int): the parameter for frequency-based filtering\n",
    "        Returns:\n",
    "            an instance of the ReviewVectorizer\n",
    "        \"\"\"\n",
    "        review_vocab = Vocabulary(add_unk=True)   # create an instance of Vocabulary class\n",
    "        rating_vocab = Vocabulary(add_unk=False)\n",
    "        \n",
    "        # Add ratings\n",
    "        for rating in sorted(set(review_df.rating)):  \n",
    "            rating_vocab.add_token(rating)            # {'negative':0, 'positive':1}\n",
    "\n",
    "        # Add top words if count > provided count\n",
    "        word_counts = Counter()\n",
    "        for review in review_df.review:\n",
    "            for word in review.split(\" \"):\n",
    "                # if word not in string.punctuation:\n",
    "                word_counts[word] += 1\n",
    "               \n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                review_vocab.add_token(word)          # {'<UNK>':0, 'apple':1, 'banana':2, ....., 'zzz':10000}\n",
    "\n",
    "        if vectorizer_type == Vectorizer_Type.TF_IDF :\n",
    "            tfidf_vectorizer = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=ngram_option)  # support unigram + bigram words\n",
    "            tfidf_vectorizer.fit(review_df.review)\n",
    "        else :\n",
    "            tfidf_vectorizer = None\n",
    "\n",
    "        return cls(review_vocab, rating_vocab, vectorizer_type, tfidf_vectorizer, enable_opinion_lexicon, use_word_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作用：\n",
    "\n",
    "负责 将文本转换为数值向量，用于机器学习模型训练。\n",
    "支持 词频 (Term Frequency) 统计 或 One-hot 编码。\n",
    "该类主要负责 文本向量化，将文本转换为模型可以使用的 数值表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, review_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            review_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (ReviewVectorizer): vectorizer instantiated from dataset\n",
    "        \"\"\"\n",
    "        self.review_df = review_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.review_df[self.review_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.review_df[self.review_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.review_df[self.review_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, review_csv, preprocessing_args, vectorizer_type, enable_opinion_lexicon, use_word_embedding, ngram_option, frequency_cutoff):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            cls: class name, i.e. ReviewDataset\n",
    "            review_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of ReviewDataset\n",
    "        \"\"\"\n",
    "        review_df = pd.read_csv(review_csv)\n",
    "        # train_review_df = review_df[review_df.split=='train']\n",
    "        train_review_df = review_df[review_df.split=='train'].copy() ### avoid too many alerts\n",
    "\n",
    "        ##### APPLY PREPROCESSING HERE\n",
    "        text_col = \"review\"\n",
    "        if preprocessing_args.remove_special_char :\n",
    "            train_review_df[text_col] = train_review_df[text_col].apply(remove_special_char)\n",
    "\n",
    "        if preprocessing_args.case_folding :\n",
    "            train_review_df[text_col] = train_review_df[text_col].apply(case_folding)\n",
    "\n",
    "        if preprocessing_args.remove_punctuation :\n",
    "            train_review_df[text_col] = train_review_df[text_col].apply(remove_punctuation)\n",
    "\n",
    "        if preprocessing_args.expand_contractions :\n",
    "            train_review_df[text_col] = train_review_df[text_col].apply(expand_contractions)\n",
    "        \n",
    "        if preprocessing_args.remove_stop_words :\n",
    "            train_review_df[text_col] = train_review_df[text_col].apply(remove_stop_words)\n",
    "\n",
    "        if preprocessing_args.wordnet_lemmatizer :\n",
    "            train_review_df[text_col] = train_review_df[text_col].apply(wordnet_lemmatizer)\n",
    "\n",
    "\n",
    "        return cls(review_df, ReviewVectorizer.from_dataframe(train_review_df, vectorizer_type, enable_opinion_lexicon, use_word_embedding, ngram_option, frequency_cutoff))\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \n",
    "        \n",
    "        Args:\n",
    "            split (str): one of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        review_vector = \\\n",
    "            self._vectorizer.vectorize(row.review)\n",
    "\n",
    "        rating_index = \\\n",
    "            self._vectorizer.rating_vocab.lookup_token(row.rating)\n",
    "\n",
    "        return {'x_data': review_vector,           #  e.g., { 'x_data': [0, 0, 1, 0, 1, ....., 0, 0],\n",
    "                'y_target': rating_index}          #          'y_target': 0  }\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size  # the floor division // rounds the result down to the nearest whole number\n",
    "    \n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    # drop_last: set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. \n",
    "    # If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False)\n",
    "    # need to use sampler option for balanced data: \n",
    "    # https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "    \n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():  # name: x_data & y_target\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model: ReviewClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewClassifier(nn.Module):\n",
    "    \"\"\" a simple perceptron based classifier \"\"\"\n",
    "    def __init__(self, num_features, hidden_dim, enable_second_hidden_layer, dropout_rate, enable_batch_norm):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int): the size of the input feature vector\n",
    "            hidden_dim   (int): the size of hidden dimension\n",
    "        \"\"\"\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.enable_second_hidden_layer = enable_second_hidden_layer\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.enable_batch_norm = enable_batch_norm\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=num_features, out_features=hidden_dim)\n",
    "        if enable_batch_norm:\n",
    "            self.batch_norm1 = nn.BatchNorm1d(hidden_dim)\n",
    "        if self.dropout_rate != 0:\n",
    "            self.dropout1 = nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "        if enable_second_hidden_layer :\n",
    "            self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2) ###base on experience from lab assignment 1\n",
    "            if enable_batch_norm:\n",
    "                self.batch_norm2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "            if self.dropout_rate != 0 :    \n",
    "                self.dropout2 = nn.Dropout(p=self.dropout_rate)\n",
    "            self.fc_final = nn.Linear(hidden_dim // 2, out_features=1)\n",
    "        else :\n",
    "            self.fc_final = nn.Linear(hidden_dim, out_features=1)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        \"\"\"The forward pass of the classifier    \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be [batch, num_features]\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be [batch]\n",
    "        \"\"\"\n",
    "        intermediate = self.fc1(x_in)            # [batch, num_features] -> [batch, hidden_dim]\n",
    "        if self.enable_batch_norm:\n",
    "            intermediate = self.batch_norm1(intermediate)  \n",
    "        intermediate = F.relu(intermediate)      # [batch, hidden_dim]\n",
    "        if self.dropout_rate != 0 :\n",
    "            intermediate = self.dropout1(intermediate)\n",
    "\n",
    "        if self.enable_second_hidden_layer:\n",
    "            # Second hidden layer\n",
    "            intermediate = self.fc2(intermediate)\n",
    "            if self.enable_batch_norm:\n",
    "                intermediate = self.batch_norm2(intermediate)\n",
    "            intermediate = F.relu(intermediate)\n",
    "            if self.dropout_rate != 0 :\n",
    "                intermediate = self.dropout2(intermediate)\n",
    "\n",
    "        y_out = self.fc_final(intermediate)           # [batch, hidden_dim] -> [batch, out_features]\n",
    "        return torch.sigmoid(y_out).squeeze()    # [batch, 1] -> [batch] (e.g., [0.3, 0.1, 0.7, 0.8, ..., 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Routine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "                train_state['early_stopping_best_val'] = loss_t  # update 'early_stopping_best_val'\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        # print(train_state['train_acc'])\n",
    "        if train_state['early_stopping_step'] >= args.early_stopping_criteria or train_state['train_acc'][-1] > 99.9 : ### if  train_state['train_acc'] > 0.999 also can stop directly\n",
    "            train_state['stop_early'] = True\n",
    "        else : \n",
    "            train_state['stop_early'] = False\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (y_pred>0.5).cpu().long()\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()  # item() to get a Python number from a tensor containing a single value\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings and some prep work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/ch3/yelp/model.pth\n",
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "current parameters are only original value.\n",
    "'''\n",
    "preprocessing_args = Namespace(\n",
    "    remove_punctuation=False,\n",
    "    remove_special_char=False,\n",
    "    case_folding=False,\n",
    "    expand_contractions=False,\n",
    "    remove_stop_words=False,\n",
    "    wordnet_lemmatizer=False,\n",
    ")\n",
    "\n",
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file='model.pth',\n",
    "    review_csv='data/yelp/reviews_with_splits_lite.csv',\n",
    "    save_dir='model_storage/ch3/yelp/',\n",
    "    # No Model hyper parameters\n",
    "    hidden_dim=20,\n",
    "    # Training hyper parameters\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=3,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    # Runtime options\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    "\n",
    "\n",
    "    ###Newly added\n",
    "    ######for vectorizer\n",
    "    vectorizer_type=Vectorizer_Type.One_Hot,\n",
    "    ngram_option=(1,1),\n",
    "    enable_opinion_lexicon=False,\n",
    "    use_word_embedding=False,\n",
    "\n",
    "    ####for ReviewClassifier\n",
    "    enable_second_hidden_layer=False,\n",
    "    dropout_rate=0,\n",
    "    enable_batch_norm=False,\n",
    "\n",
    "    ####for optimizer\n",
    "    weight_decay=0, ##change to 0 to disable\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs ; creat dirs if they don't exist\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(preprocessing_args, args) :\n",
    "    # print(\"Loading dataset and creating vectorizer\")\n",
    "    # create dataset and vectorizer\n",
    "    dataset = ReviewDataset.load_dataset_and_make_vectorizer(args.review_csv, preprocessing_args, \n",
    "                                                            vectorizer_type=args.vectorizer_type,\n",
    "                                                            enable_opinion_lexicon=args.enable_opinion_lexicon,\n",
    "                                                            use_word_embedding=args.use_word_embedding,\n",
    "                                                            ngram_option=args.ngram_option,\n",
    "                                                            frequency_cutoff=args.frequency_cutoff)\n",
    "\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "    if args.vectorizer_type == Vectorizer_Type.TF_IDF :\n",
    "        num_features = len(vectorizer.tfidf_vectorizer.vocabulary_)\n",
    "        if args.enable_opinion_lexicon :\n",
    "            num_features = num_features + 2\n",
    "        if args.use_word_embedding :\n",
    "            num_features = num_features + 300\n",
    "            print(f'number of features => {num_features}')\n",
    "        classifier = ReviewClassifier(num_features=num_features, hidden_dim=args.hidden_dim, enable_second_hidden_layer=args.enable_second_hidden_layer, dropout_rate=args.dropout_rate, enable_batch_norm=args.enable_batch_norm)\n",
    "    else :\n",
    "        num_features=len(vectorizer.review_vocab) ### original code\n",
    "        if args.enable_opinion_lexicon :\n",
    "            num_features = num_features + 2\n",
    "        if args.use_word_embedding :\n",
    "            num_features = num_features + 300\n",
    "        print(f'number of features => {num_features}')\n",
    "        classifier = ReviewClassifier(num_features=num_features, hidden_dim=args.hidden_dim, enable_second_hidden_layer=args.enable_second_hidden_layer, dropout_rate=args.dropout_rate, enable_batch_norm=args.enable_batch_norm)\n",
    "    return dataset, vectorizer, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#str(vectorizer.review_vocab), str(vectorizer.rating_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "1. lr_scheduler.ReduceLROnPlateau(): Reduce learning rate when a metric has stopped improving. Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This scheduler reads a metrics quantity and if no improvement is seen for a ‘patience’ number of epochs, the learning rate is reduced.(https://pytorch.org/docs/stable/optim.html)\n",
    "> - mode (str) – One of min, max. In min mode, lr will be reduced when the quantity monitored has stopped decreasing; in max mode it will be reduced when the quantity monitored has stopped increasing. Default: ‘min’.<br>\n",
    ">- factor (float) – Factor by which the learning rate will be reduced. new_lr = lr * factor. Default: 0.1.<br>\n",
    ">- patience (int) – Number of epochs with no improvement after which learning rate will be reduced. For example, if patience = 2, then we will ignore the first 2 epochs with no improvement, and will only decrease the LR after the 3rd epoch if the loss still hasn’t improved then. Default: 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     30
    ]
   },
   "outputs": [],
   "source": [
    "def train_model(preprocessing_args, args, dataset, classifier, disable_tqdm=False) :\n",
    "    classifier = classifier.to(args.device)\n",
    "\n",
    "    loss_func = nn.BCELoss()\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                    mode='min', factor=0.1,\n",
    "                                                    patience=10) # Reduce learning rate when a metric has stopped improving.\n",
    "\n",
    "    train_state = make_train_state(args)\n",
    "\n",
    "    epoch_bar = tqdm(desc='training routine', total=args.num_epochs, position=0, disable=disable_tqdm)  # progress bar\n",
    "\n",
    "    dataset.set_split('train')\n",
    "    train_bar = tqdm(desc='split=train', total=dataset.get_num_batches(args.batch_size), position=1, leave=True, disable=disable_tqdm)\n",
    "\n",
    "    dataset.set_split('val')\n",
    "    val_bar = tqdm(desc='split=val', total=dataset.get_num_batches(args.batch_size), position=1, leave=True, disable=disable_tqdm)\n",
    "\n",
    "    try:\n",
    "        for epoch_index in range(args.num_epochs):\n",
    "            train_state['epoch_index'] = epoch_index\n",
    "\n",
    "            # Iterate over training dataset\n",
    "\n",
    "            # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "            dataset.set_split('train')\n",
    "            batch_generator = generate_batches(dataset, \n",
    "                                            batch_size=args.batch_size, \n",
    "                                            device=args.device)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            classifier.train()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                # the training routine is these 5 steps:\n",
    "\n",
    "                # --------------------------------------\n",
    "                # step 1. zero the gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # step 2. compute the output\n",
    "                y_pred = classifier(x_in=batch_dict['x_data'].float())  # [batch, num_features] -> [batch]\n",
    "\n",
    "                # step 3. compute the loss\n",
    "                loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # step 4. use loss to produce gradients\n",
    "                loss.backward()\n",
    "\n",
    "                # step 5. use optimizer to take gradient step\n",
    "                optimizer.step()\n",
    "                # -----------------------------------------\n",
    "                # compute the accuracy\n",
    "                acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "                \n",
    "                # update bar\n",
    "                train_bar.set_postfix(loss=running_loss, \n",
    "                                    acc=running_acc, \n",
    "                                    epoch=epoch_index)\n",
    "                train_bar.update()\n",
    "            \n",
    "            train_state['train_loss'].append(running_loss)  # train_loss for each epoch\n",
    "            train_state['train_acc'].append(running_acc)    # train_acc for each epoch\n",
    "            \n",
    "            # Iterate over val dataset\n",
    "\n",
    "            # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "            dataset.set_split('val')\n",
    "            batch_generator = generate_batches(dataset, \n",
    "                                            batch_size=args.batch_size, \n",
    "                                            device=args.device)\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "            classifier.eval()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "                # compute the output\n",
    "                y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "\n",
    "                # step 3. compute the loss\n",
    "                loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # compute the accuracy\n",
    "                acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "                \n",
    "                val_bar.set_postfix(loss=running_loss, \n",
    "                                    acc=running_acc, \n",
    "                                    epoch=epoch_index)\n",
    "                val_bar.update()\n",
    "\n",
    "            train_state['val_loss'].append(running_loss)  # val_loss for each epoch\n",
    "            train_state['val_acc'].append(running_acc)    # val_acc for each epoch\n",
    "\n",
    "            train_state = update_train_state(args=args, model=classifier,\n",
    "                                            train_state=train_state)\n",
    "\n",
    "            scheduler.step(train_state['val_loss'][-1])  # adjust learning rate\n",
    "\n",
    "            if train_state['stop_early']:\n",
    "                break\n",
    "\n",
    "            train_bar.n = 0   # reset number of finished iterations\n",
    "            val_bar.n = 0\n",
    "            epoch_bar.update()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting loop\")\n",
    "    return train_state, loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_loss_and_acc(train_state, title) :\n",
    "    epochs = range(1, len(train_state['train_acc']) + 1)\n",
    "    fig, ax1 = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "    fig.suptitle(title, fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    ax1[0].plot(epochs, train_state['train_loss'], 'bo-', label='Training Loss')\n",
    "    ax1[0].plot(epochs, train_state['val_loss'], 'r-', label='Validation Loss') \n",
    "    ax1[0].set_title('Training and Validation Loss')\n",
    "    ax1[0].set_xlabel('Epochs')\n",
    "    ax1[0].set_ylabel('Loss')\n",
    "    ax1[0].legend()\n",
    "\n",
    "    ax1[1].plot(epochs, train_state['train_acc'], 'go-', label='Training Accuracy')\n",
    "    ax1[1].plot(epochs, train_state['val_acc'], 'm-', label='Validation Accuracy') \n",
    "    ax1[1].set_title('Training and Validation Accuracy')\n",
    "    ax1[1].set_xlabel('Epochs')\n",
    "    ax1[1].set_ylabel('Accuracy')\n",
    "    ax1[1].legend()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.tight_layout() \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "def check_test_data(preprocessing_args, args, dataset, classifier, train_state, loss_func, enable_print_info=True) :\n",
    "    classifier.load_state_dict(torch.load(train_state['model_filename'], weights_only=False))\n",
    "    classifier = classifier.to(args.device)\n",
    "\n",
    "    dataset.set_split('test')\n",
    "    batch_generator = generate_batches(dataset, \n",
    "                                    batch_size=args.batch_size, \n",
    "                                    device=args.device)\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "    y_pred_list = []    # store predicted values for confusion matrix\n",
    "    y_target_list = []  # ground truth value\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # compute the output\n",
    "        y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
    "        \n",
    "        # store predicted values and ground truth values for calculating confusion matrix\n",
    "        y_pred_list.extend((y_pred>0.5).cpu().long().numpy())\n",
    "        y_target_list.extend(batch_dict['y_target'].cpu().numpy())\n",
    "        \n",
    "        # compute the loss\n",
    "        loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "        # compute the accuracy\n",
    "        acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "    train_state['test_loss'] = running_loss\n",
    "    train_state['test_acc'] = running_acc\n",
    "\n",
    "    if enable_print_info: \n",
    "        print(\"==================================dividing line==================================\")\n",
    "        print(\"Test loss: {:.3f}\".format(train_state['test_loss']))\n",
    "        print(\"Test Accuracy: {:.2f}\".format(train_state['test_acc']))\n",
    "\n",
    "    if enable_print_info: \n",
    "        rating_classes = []\n",
    "        for i in range(len(dataset._vectorizer.rating_vocab)):\n",
    "            rating_classes.append(dataset._vectorizer.rating_vocab.lookup_index(i))\n",
    "        # print(\"==================================dividing line=======================\")\n",
    "        # print(rating_classes)\n",
    "\n",
    "    if enable_print_info: \n",
    "        cm = confusion_matrix(y_target_list, y_pred_list)\n",
    "        cm_df = pd.DataFrame(cm.T, index=rating_classes, columns=rating_classes)\n",
    "        cm_df.index.name = 'Predicted'\n",
    "        cm_df.columns.name = 'True'\n",
    "        print(\"==================================dividing line==================================\")\n",
    "        print(cm_df)\n",
    "\n",
    "        print(\"==================================dividing line==================================\")\n",
    "        print(classification_report(y_target_list, y_pred_list))\n",
    "\n",
    "    return train_state['test_acc'], train_state['test_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### orignial result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features => 2264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a522f7a28aa42b487edd0f16b1236e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72eb749cba5644abb7f1d75656804421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808e9b96c0e54ce28ac8f4744077c5f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================dividing line==================================\n",
      "Test loss: 0.352\n",
      "Test Accuracy: 86.52\n",
      "==================================dividing line==================================\n",
      "True       negative  positive\n",
      "Predicted                    \n",
      "negative        454        80\n",
      "positive         58       432\n",
      "==================================dividing line==================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.89      0.87       512\n",
      "           1       0.88      0.84      0.86       512\n",
      "\n",
      "    accuracy                           0.87      1024\n",
      "   macro avg       0.87      0.87      0.87      1024\n",
      "weighted avg       0.87      0.87      0.87      1024\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(86.5234375, 0.351644329726696)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, vectorizer, classifier = init(preprocessing_args, args)\n",
    "train_state, loss_func = train_model(preprocessing_args, args, dataset=dataset, classifier=classifier, disable_tqdm=False)\n",
    "# draw_loss_and_acc(train_state, \"ORIGINAL\")\n",
    "check_test_data(preprocessing_args, args, dataset, classifier, train_state, loss_func, enable_print_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1\n",
    "### modify any 1 parameter and get result here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/ch3/yelp/model.pth\n",
      "Using CUDA: True\n",
      "number of features => 2264\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb932a8f6bb453e90f2547c702f3a6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cbadd8a4814a2f8b2cc665816d1b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2b55f3b4b54dbdbb8fe0740795251e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.71875\n",
      "0.35127186402678495\n"
     ]
    }
   ],
   "source": [
    "preprocessing_args = Namespace(\n",
    "    remove_punctuation=False, #True\n",
    "    remove_special_char=False, #True\n",
    "    case_folding=False, #True\n",
    "    expand_contractions=False, #True\n",
    "    remove_stop_words=False, #True\n",
    "    wordnet_lemmatizer=False, #True\n",
    ")\n",
    "\n",
    "args = Namespace(\n",
    "    frequency_cutoff=25, #0, 50\n",
    "    model_state_file='model.pth',\n",
    "    review_csv='data/yelp/reviews_with_splits_lite.csv',\n",
    "    save_dir='model_storage/ch3/yelp/',\n",
    "    hidden_dim=20, #10, 30\n",
    "    batch_size=128, #64, 256\n",
    "    early_stopping_criteria=3,\n",
    "    learning_rate=0.001, #0.0001, 0.01\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False, \n",
    "    vectorizer_type=Vectorizer_Type.One_Hot, #Vectorizer_Type.Term_Freq,  Vectorizer_Type.TF_IDF, One_Hot\n",
    "    ngram_option=(1,1), #(1,2) only apply for Vectorizer_Type.TF_IDF\n",
    "    enable_opinion_lexicon=False, #True\n",
    "    use_word_embedding=False,  #True\n",
    "    enable_second_hidden_layer=False,  #True\n",
    "    dropout_rate=0,  #0.25, 0.5\n",
    "    enable_batch_norm=False,  #True\n",
    "    weight_decay=0.0001, #0.0001?\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs ; creat dirs if they don't exist\n",
    "handle_dirs(args.save_dir)\n",
    "\n",
    "# Train model\n",
    "dataset, vectorizer, classifier = init(preprocessing_args, args)\n",
    "train_state, loss_func = train_model(preprocessing_args, args, dataset=dataset, classifier=classifier)\n",
    "# Compute accuracy\n",
    "test_acc, test_loss = check_test_data(preprocessing_args, args, dataset, classifier, train_state, loss_func, enable_print_info=False)\n",
    "print(test_acc)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-28 23:46:24,235] A new study created in memory with name: no-name-0b709e4d-73b3-4e7a-b724-944fdd046189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features => 2441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-28 23:47:35,502] Trial 0 finished with value: 86.03515625 and parameters: {'case_folding': True, 'vectorizer_type': 'term-freq', 'frequency_cutoff': 22, 'learning_rate': 0.00016874085710901152, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': False, 'dropout_rate': 0.25, 'weight_decay': 0.0001}. Best is trial 0 with value: 86.03515625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features => 3220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-28 23:48:24,278] Trial 1 finished with value: 86.23046875 and parameters: {'case_folding': True, 'vectorizer_type': 'term-freq', 'frequency_cutoff': 15, 'learning_rate': 0.0003705527874656162, 'enable_opinion_lexicon': True, 'enable_second_hidden_layer': True, 'dropout_rate': 0.21000000000000002, 'weight_decay': 1e-05}. Best is trial 1 with value: 86.23046875.\n",
      "[I 2025-02-28 23:55:38,802] Trial 2 finished with value: 90.33203125 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 24, 'learning_rate': 0.0007155080630940906, 'enable_opinion_lexicon': True, 'enable_second_hidden_layer': True, 'dropout_rate': 0.24000000000000002, 'weight_decay': 0}. Best is trial 2 with value: 90.33203125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features => 2204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-28 23:56:16,137] Trial 3 finished with value: 85.83984375 and parameters: {'case_folding': True, 'vectorizer_type': 'term-freq', 'frequency_cutoff': 25, 'learning_rate': 0.0004715706226575885, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.11, 'weight_decay': 0.0001}. Best is trial 2 with value: 90.33203125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features => 19707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-28 23:57:32,343] Trial 4 finished with value: 87.890625 and parameters: {'case_folding': False, 'vectorizer_type': 'term-freq', 'frequency_cutoff': 1, 'learning_rate': 0.00020419060709349414, 'enable_opinion_lexicon': True, 'enable_second_hidden_layer': True, 'dropout_rate': 0.23, 'weight_decay': 1e-05}. Best is trial 2 with value: 90.33203125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features => 3072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-28 23:58:34,835] Trial 5 finished with value: 86.42578125 and parameters: {'case_folding': True, 'vectorizer_type': 'term-freq', 'frequency_cutoff': 16, 'learning_rate': 0.00023779451894446283, 'enable_opinion_lexicon': True, 'enable_second_hidden_layer': False, 'dropout_rate': 0.13, 'weight_decay': 1e-05}. Best is trial 2 with value: 90.33203125.\n",
      "[I 2025-03-01 00:02:14,173] Trial 6 finished with value: 91.40625 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 14, 'learning_rate': 0.0005369431197864281, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.13, 'weight_decay': 1e-05}. Best is trial 6 with value: 91.40625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features => 5870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 00:03:38,933] Trial 7 finished with value: 88.28125 and parameters: {'case_folding': False, 'vectorizer_type': 'term-freq', 'frequency_cutoff': 7, 'learning_rate': 0.0001693296035864477, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': False, 'dropout_rate': 0.23, 'weight_decay': 1e-05}. Best is trial 6 with value: 91.40625.\n",
      "[I 2025-03-01 00:11:10,445] Trial 8 finished with value: 88.671875 and parameters: {'case_folding': False, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,1', 'frequency_cutoff': 2, 'learning_rate': 0.00013852585520221822, 'enable_opinion_lexicon': True, 'enable_second_hidden_layer': False, 'dropout_rate': 0.12000000000000001, 'weight_decay': 0.0001}. Best is trial 6 with value: 91.40625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features => 3988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 00:12:12,229] Trial 9 finished with value: 88.28125 and parameters: {'case_folding': False, 'vectorizer_type': 'term-freq', 'frequency_cutoff': 12, 'learning_rate': 0.00027532071795832334, 'enable_opinion_lexicon': True, 'enable_second_hidden_layer': False, 'dropout_rate': 0.17, 'weight_decay': 0.0001}. Best is trial 6 with value: 91.40625.\n",
      "[I 2025-03-01 00:15:12,410] Trial 10 finished with value: 91.30859375 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 9, 'learning_rate': 0.0009037559585503423, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.16, 'weight_decay': 0}. Best is trial 6 with value: 91.40625.\n",
      "[I 2025-03-01 00:17:12,420] Trial 11 finished with value: 91.40625 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 9, 'learning_rate': 0.0009932565236384094, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.15000000000000002, 'weight_decay': 0}. Best is trial 6 with value: 91.40625.\n",
      "[I 2025-03-01 00:20:36,640] Trial 12 finished with value: 91.40624999999999 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 18, 'learning_rate': 0.0005816941168539892, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.14, 'weight_decay': 0}. Best is trial 6 with value: 91.40625.\n",
      "[I 2025-03-01 00:23:56,934] Trial 13 finished with value: 91.30859375 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 7, 'learning_rate': 0.0009547148539578927, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.19, 'weight_decay': 0}. Best is trial 6 with value: 91.40625.\n",
      "[I 2025-03-01 00:27:55,445] Trial 14 finished with value: 91.50390625 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 11, 'learning_rate': 0.00045301357700228335, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.15000000000000002, 'weight_decay': 1e-05}. Best is trial 14 with value: 91.50390625.\n",
      "[I 2025-03-01 00:30:27,602] Trial 15 finished with value: 88.57421875 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,1', 'frequency_cutoff': 20, 'learning_rate': 0.0003757773710902332, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.1, 'weight_decay': 1e-05}. Best is trial 14 with value: 91.50390625.\n",
      "[I 2025-03-01 00:36:02,773] Trial 16 finished with value: 90.8203125 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 12, 'learning_rate': 0.0005039196826849292, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.19, 'weight_decay': 1e-05}. Best is trial 14 with value: 91.50390625.\n",
      "[I 2025-03-01 00:39:11,706] Trial 17 finished with value: 91.6015625 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 14, 'learning_rate': 0.0006675308153888444, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.14, 'weight_decay': 1e-05}. Best is trial 17 with value: 91.6015625.\n",
      "[I 2025-03-01 00:40:26,674] Trial 18 finished with value: 89.6484375 and parameters: {'case_folding': False, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,1', 'frequency_cutoff': 4, 'learning_rate': 0.0007085504703674462, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.18, 'weight_decay': 1e-05}. Best is trial 17 with value: 91.6015625.\n",
      "[I 2025-03-01 00:45:46,761] Trial 19 finished with value: 91.6015625 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 18, 'learning_rate': 0.000384916045047933, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.15000000000000002, 'weight_decay': 1e-05}. Best is trial 17 with value: 91.6015625.\n",
      "[I 2025-03-01 00:59:25,339] Trial 20 finished with value: 91.11328125 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 18, 'learning_rate': 0.00010446115689088757, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.16, 'weight_decay': 1e-05}. Best is trial 17 with value: 91.6015625.\n",
      "[I 2025-03-01 01:04:24,549] Trial 21 finished with value: 91.50390625 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 11, 'learning_rate': 0.0003757520955557877, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.15000000000000002, 'weight_decay': 1e-05}. Best is trial 17 with value: 91.6015625.\n",
      "[I 2025-03-01 01:07:32,325] Trial 22 finished with value: 91.69921875 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 18, 'learning_rate': 0.0007031768362678713, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.14, 'weight_decay': 1e-05}. Best is trial 22 with value: 91.69921875.\n",
      "[I 2025-03-01 01:10:11,857] Trial 23 finished with value: 91.796875 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 18, 'learning_rate': 0.0007098127794214199, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.13, 'weight_decay': 1e-05}. Best is trial 23 with value: 91.796875.\n",
      "[I 2025-03-01 01:12:51,363] Trial 24 finished with value: 91.69921875000001 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 21, 'learning_rate': 0.0007195745738418776, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.12000000000000001, 'weight_decay': 1e-05}. Best is trial 23 with value: 91.796875.\n",
      "[I 2025-03-01 01:14:09,418] Trial 25 finished with value: 89.453125 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,1', 'frequency_cutoff': 21, 'learning_rate': 0.0007964054816876642, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.1, 'weight_decay': 1e-05}. Best is trial 23 with value: 91.796875.\n",
      "[I 2025-03-01 01:18:00,502] Trial 26 finished with value: 89.94140625 and parameters: {'case_folding': False, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 22, 'learning_rate': 0.0006265404327686316, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': False, 'dropout_rate': 0.12000000000000001, 'weight_decay': 1e-05}. Best is trial 23 with value: 91.796875.\n",
      "[I 2025-03-01 01:20:38,739] Trial 27 finished with value: 91.89453125 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 19, 'learning_rate': 0.0008201393729415589, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.12000000000000001, 'weight_decay': 1e-05}. Best is trial 27 with value: 91.89453125.\n",
      "[I 2025-03-01 01:23:39,535] Trial 28 finished with value: 91.2109375 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 20, 'learning_rate': 0.000829075708199397, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.11, 'weight_decay': 0.0001}. Best is trial 27 with value: 91.89453125.\n",
      "[I 2025-03-01 01:25:48,378] Trial 29 finished with value: 88.57421875 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,1', 'frequency_cutoff': 23, 'learning_rate': 0.0008121212943265351, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': False, 'dropout_rate': 0.12000000000000001, 'weight_decay': 1e-05}. Best is trial 27 with value: 91.89453125.\n",
      "[I 2025-03-01 01:30:22,413] Trial 30 finished with value: 90.4296875 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 16, 'learning_rate': 0.0005722473486745389, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.11, 'weight_decay': 0.0001}. Best is trial 27 with value: 91.89453125.\n",
      "[I 2025-03-01 01:33:00,129] Trial 31 finished with value: 91.89453125 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 20, 'learning_rate': 0.0007242542181661543, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.13, 'weight_decay': 1e-05}. Best is trial 27 with value: 91.89453125.\n",
      "[I 2025-03-01 01:35:35,896] Trial 32 finished with value: 91.9921875 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 20, 'learning_rate': 0.000787568582846762, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.13, 'weight_decay': 1e-05}. Best is trial 32 with value: 91.9921875.\n",
      "[I 2025-03-01 01:38:19,290] Trial 33 finished with value: 92.1875 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 19, 'learning_rate': 0.0008605894497174804, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.13, 'weight_decay': 1e-05}. Best is trial 33 with value: 92.1875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features => 2206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 01:38:45,262] Trial 34 finished with value: 86.9140625 and parameters: {'case_folding': True, 'vectorizer_type': 'term-freq', 'frequency_cutoff': 25, 'learning_rate': 0.0008613995469643083, 'enable_opinion_lexicon': True, 'enable_second_hidden_layer': True, 'dropout_rate': 0.1, 'weight_decay': 1e-05}. Best is trial 33 with value: 92.1875.\n",
      "[I 2025-03-01 01:42:29,945] Trial 35 finished with value: 91.30859375 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 23, 'learning_rate': 0.0004337201421737151, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.13, 'weight_decay': 1e-05}. Best is trial 33 with value: 92.1875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features => 3072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 01:43:12,117] Trial 36 finished with value: 85.83984375 and parameters: {'case_folding': True, 'vectorizer_type': 'term-freq', 'frequency_cutoff': 16, 'learning_rate': 0.00031416401398126495, 'enable_opinion_lexicon': True, 'enable_second_hidden_layer': True, 'dropout_rate': 0.11, 'weight_decay': 1e-05}. Best is trial 33 with value: 92.1875.\n",
      "[I 2025-03-01 01:47:12,848] Trial 37 finished with value: 91.50390625 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 20, 'learning_rate': 0.000616486689676606, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.17, 'weight_decay': 1e-05}. Best is trial 33 with value: 92.1875.\n",
      "[I 2025-03-01 01:50:14,541] Trial 38 finished with value: 90.13671875 and parameters: {'case_folding': False, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 24, 'learning_rate': 0.0007695635970074278, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': False, 'dropout_rate': 0.13, 'weight_decay': 0}. Best is trial 33 with value: 92.1875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features => 3220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 01:50:45,129] Trial 39 finished with value: 86.23046875 and parameters: {'case_folding': True, 'vectorizer_type': 'term-freq', 'frequency_cutoff': 15, 'learning_rate': 0.0005194113425160145, 'enable_opinion_lexicon': True, 'enable_second_hidden_layer': True, 'dropout_rate': 0.14, 'weight_decay': 1e-05}. Best is trial 33 with value: 92.1875.\n",
      "[I 2025-03-01 01:55:02,454] Trial 40 finished with value: 90.91796874999999 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 19, 'learning_rate': 0.0009984260640908595, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.2, 'weight_decay': 0.0001}. Best is trial 33 with value: 92.1875.\n",
      "[I 2025-03-01 01:57:36,403] Trial 41 finished with value: 91.50390625 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 17, 'learning_rate': 0.0008714673948903029, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.13, 'weight_decay': 1e-05}. Best is trial 33 with value: 92.1875.\n",
      "[I 2025-03-01 02:02:38,748] Trial 42 finished with value: 91.99218749999999 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 22, 'learning_rate': 0.0007504175784539467, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.25, 'weight_decay': 1e-05}. Best is trial 33 with value: 92.1875.\n",
      "[I 2025-03-01 02:09:08,132] Trial 43 finished with value: 90.8203125 and parameters: {'case_folding': True, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 22, 'learning_rate': 0.000629836494856639, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.21000000000000002, 'weight_decay': 1e-05}. Best is trial 33 with value: 92.1875.\n",
      "[I 2025-03-01 02:14:26,849] Trial 44 finished with value: 91.9921875 and parameters: {'case_folding': False, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 22, 'learning_rate': 0.000766162420486137, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.23, 'weight_decay': 1e-05}. Best is trial 33 with value: 92.1875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features => 2345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-01 02:14:54,391] Trial 45 finished with value: 87.01171875 and parameters: {'case_folding': False, 'vectorizer_type': 'term-freq', 'frequency_cutoff': 24, 'learning_rate': 0.0008996868833562509, 'enable_opinion_lexicon': True, 'enable_second_hidden_layer': False, 'dropout_rate': 0.24000000000000002, 'weight_decay': 1e-05}. Best is trial 33 with value: 92.1875.\n",
      "[I 2025-03-01 02:20:39,669] Trial 46 finished with value: 91.11328125 and parameters: {'case_folding': False, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 25, 'learning_rate': 0.0005605680980863922, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.25, 'weight_decay': 0}. Best is trial 33 with value: 92.1875.\n",
      "[I 2025-03-01 02:33:11,377] Trial 47 finished with value: 91.6015625 and parameters: {'case_folding': False, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 23, 'learning_rate': 0.0001852994753578446, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.22, 'weight_decay': 1e-05}. Best is trial 33 with value: 92.1875.\n",
      "[I 2025-03-01 02:34:29,924] Trial 48 finished with value: 89.74609375 and parameters: {'case_folding': False, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,1', 'frequency_cutoff': 21, 'learning_rate': 0.0007857140073150598, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': True, 'dropout_rate': 0.25, 'weight_decay': 0.0001}. Best is trial 33 with value: 92.1875.\n",
      "[I 2025-03-01 02:37:47,708] Trial 49 finished with value: 90.625 and parameters: {'case_folding': False, 'vectorizer_type': 'tf-idf', 'ngram_option': '1,2', 'frequency_cutoff': 14, 'learning_rate': 0.0009180225803280563, 'enable_opinion_lexicon': False, 'enable_second_hidden_layer': False, 'dropout_rate': 0.25, 'weight_decay': 1e-05}. Best is trial 33 with value: 92.1875.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['params_enable_drop_out'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 83\u001b[0m\n\u001b[0;32m     80\u001b[0m df \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mtrials_dataframe()\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Extract top 10 best trials with all parameters\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m top_10_trials \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Test Accuracy\u001b[39;49;00m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams_case_folding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams_frequency_cutoff\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams_learning_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams_vectorizer_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams_ngram_option\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams_enable_opinion_lexicon\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams_enable_second_hidden_layer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams_enable_drop_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparams_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     94\u001b[0m \u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTop 10 Best Hyperparameter Combinations:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, trial \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(top_10_trials):\n",
      "File \u001b[1;32mc:\\Users\\Zhou&Wan\\Documents\\GitHub\\Text-Mining\\gaoqiang_env\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Zhou&Wan\\Documents\\GitHub\\Text-Mining\\gaoqiang_env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Zhou&Wan\\Documents\\GitHub\\Text-Mining\\gaoqiang_env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['params_enable_drop_out'] not in index\""
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "vectorizer_type_mapping = {\n",
    "    \"one-hot\": Vectorizer_Type.One_Hot,\n",
    "    \"term-freq\": Vectorizer_Type.Term_Freq,\n",
    "    \"tf-idf\": Vectorizer_Type.TF_IDF\n",
    "}\n",
    "ngram_options = [\"1,1\", \"1,2\"] ##add 1,3?\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    preprocessing_args = Namespace(\n",
    "        remove_special_char=False,\n",
    "        case_folding=trial.suggest_categorical(\"case_folding\", [True, False]),\n",
    "        remove_punctuation=False,\n",
    "        expand_contractions=False,\n",
    "        remove_stop_words=False,\n",
    "        wordnet_lemmatizer=False)\n",
    "\n",
    "    vectorizer_type_str = trial.suggest_categorical(\"vectorizer_type\", [\"term-freq\", \"tf-idf\"])\n",
    "    if vectorizer_type_str == \"tf-idf\": ##only use this parameter when vectorizer_type = tf-idf\n",
    "        ngram_str = trial.suggest_categorical(\"ngram_option\", ngram_options)\n",
    "        ngram_option = tuple(map(int, ngram_str.split(',')))\n",
    "    else:\n",
    "        ngram_option = None\n",
    "\n",
    "    args = Namespace(\n",
    "        model_state_file='model.pth',\n",
    "        review_csv='data/yelp/reviews_with_splits_lite.csv',\n",
    "        save_dir='model_storage/ch3/yelp/',\n",
    "        early_stopping_criteria=5,\n",
    "        num_epochs=100,\n",
    "        seed=1337,\n",
    "        catch_keyboard_interrupt=True,\n",
    "        cuda=True,\n",
    "        expand_filepaths_to_save_dir=True,\n",
    "        reload_from_files=False,\n",
    "\n",
    "        frequency_cutoff=trial.suggest_int(\"frequency_cutoff\", 0, 25, step=1),\n",
    "        learning_rate=trial.suggest_float(\"learning_rate\", 0.0001, 0.001, log=True), # search in log space instead of uniform sampling.\n",
    "        hidden_dim=20,\n",
    "        batch_size=128, ###i think this no need to check\n",
    "        # vectorizer_type=trial.suggest_categorical(\"vectorizer_type\", [Vectorizer_Type.One_Hot, Vectorizer_Type.TF_IDF, Vectorizer_Type.Term_Freq]),\n",
    "        vectorizer_type=vectorizer_type_mapping[vectorizer_type_str],  ## prevent too much warning\n",
    "        ngram_option=ngram_option,\n",
    "        enable_opinion_lexicon=trial.suggest_categorical(\"enable_opinion_lexicon\", [True, False]),\n",
    "        use_word_embedding=False,\n",
    "        enable_second_hidden_layer=trial.suggest_categorical(\"enable_second_hidden_layer\", [True, False]),\n",
    "        dropout_rate=trial.suggest_float(\"dropout_rate\", 0.1, 0.25, step=0.01), \n",
    "        enable_batch_norm=False,\n",
    "        weight_decay=trial.suggest_categorical(\"weight_decay\", [0, 0.0001, 0.00001]),\n",
    "    )\n",
    "\n",
    "    # if args.expand_filepaths_to_save_dir:\n",
    "    #     args.model_state_file = os.path.join(args.save_dir,\n",
    "    #                                         args.model_state_file)    \n",
    "    #     print(\"Expanded filepaths: \")\n",
    "    #     print(\"\\t{}\".format(args.model_state_file))\n",
    "    if not torch.cuda.is_available():\n",
    "        args.cuda = False\n",
    "    # print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "    args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    # Set seed for reproducibility\n",
    "    set_seed_everywhere(args.seed, args.cuda)\n",
    "    # handle dirs ; creat dirs if they don't exist\n",
    "    handle_dirs(args.save_dir)\n",
    "\n",
    "    # Train model\n",
    "    dataset, vectorizer, classifier = init(preprocessing_args, args)\n",
    "    train_state, loss_func = train_model(preprocessing_args, args, dataset=dataset, classifier=classifier, disable_tqdm=True)\n",
    "    # Compute accuracy\n",
    "    test_acc, test_loss = check_test_data(preprocessing_args, args, dataset, classifier, train_state, loss_func, enable_print_info=False)\n",
    "    return test_acc  # Optuna will maximize this value\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "df = study.trials_dataframe().sort_values(\"value\", ascending=False).head(10)\n",
    "\n",
    "# Extract top 10 best trials with all parameters\n",
    "top_10_trials = df[[\n",
    "    \"value\",  # Test Accuracy\n",
    "    \"params_case_folding\",  \n",
    "    \"params_frequency_cutoff\", \n",
    "    \"params_learning_rate\",\n",
    "    \"params_vectorizer_type\",\n",
    "    \"params_ngram_option\",\n",
    "    \"params_enable_opinion_lexicon\",\n",
    "    \"params_enable_second_hidden_layer\",\n",
    "    \"params_drop_out\",\n",
    "    \"params_weight_decay\"\n",
    "]].to_dict(orient=\"records\")\n",
    "\n",
    "print(\"\\nTop 10 Best Hyperparameter Combinations:\")\n",
    "for i, trial in enumerate(top_10_trials):\n",
    "    print(f\"\\n🔹 **Rank {i+1}:** Accuracy = {trial['value']:.4f}\")\n",
    "    print({key.replace(\"params_\", \"\"): value for key, value in trial.items() if key != \"value\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaoqiang_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "156px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "5",
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
